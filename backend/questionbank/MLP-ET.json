[
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "__________ generates a bunch of normally-distributed clusters of points with specific mean and standard deviations for each cluster.",
        "options": {
            "A": "sklearn.datasets.make_clusters()",
            "B": "sklearn.datasets.make_centers()",
            "C": "sklearn.datasets.make_normal_clusters()",
            "D": "sklearn.datasets.make_blobs()"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Why is data preprocessing necessary?",
        "options": {
            "A": "Some columns have values only between 0 and 1",
            "B": "The data is divided into multiple types of files i.e. html, csv, tsv, etc.",
            "C": "The data has only numbers in all the columns."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Which of the following is correct with respect to R2-score?",
        "options": {
            "A": "R2 score is always positive and it may go up to infinity.",
            "B": "R2 score is always positive, but it ranges between 0 and 1 only.",
            "C": "R2 score can be negative. That happens if our model is worse than the mean model.",
            "D": "R2 score can be negative. That happens if the mean model is worse than our model."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.metrics import confusion_matrix\ny_true = [\"Rainy\", \"Sunny\", \"Sunny\", \"Rainy\", \"Sunny\", \"Rainy\"]\ny_pred = [\"Sunny\", \"Sunny\", \"Rainy\", \"Rainy\", \"Sunny\", \"Sunny\"]\nconfusion_matrix(y_true, y_pred, labels=[\"Rainy\", \"Sunny\"])",
        "question": "Which options represent the output of the following code block?",
        "options": {
            "A": "array([[2, 1],\n       [2, 1]])",
            "B": "array([[2, 1],\n       [1, 2]])",
            "C": "array([[1, 2],\n       [1, 2]])",
            "D": "array([[1, 2],\n       [2, 1]])"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.datasets import make_regression\nX, y = make_regression(n_samples = 10, n_features = 3)\n\nfrom sklearn.model_selection import LeavePOut\nlpo = LeavePOut(p = 2)\ncount = 0\nfor train, test in lpo.split(X):\n    print(train, test)\n    count += 1\nprint(count)",
        "question": "What will be the value of the 'count'?",
        "options": {
            "A": "20",
            "B": "30",
            "C": "45",
            "D": "15"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "X = ['a', 'b', 'c', 'd', 'e', 'f']\nfrom sklearn.model_selection import RepeatedKFold\nrkf = RepeatedKFold(n_splits = 3, n_repeats = 2, random_state = 10)\nfor train, test in rkf.split(X):\n    print(train, test)",
        "question": "Which of the following may be the correct output of the above code?",
        "options": {
            "A": "[0 1] [3 4] [2 5]\n[1 2] [4 5] [0 3]\n[0 2] [3 5] [1 4]\n[1 3] [4 5] [0 2]\n[0 2] [3 4] [1 5]\n[0 1] [2 5] [3 4]",
            "B": "[0 1 3 4] [3 5]\n[1 2 4 5] [0 2]\n[0 2 3 5] [1 5]\n[1 3 4 5] [0 4]\n[0 2 3 4] [1 0]\n[0 1 2 5] [3 2]",
            "C": "[0 1 3 4] [2 5]\n[1 2 4 5] [0 3]\n[0 2 3 5] [1 4]\n[1 3 4 5] [0 2]\n[0 2 3 4] [1 5]\n[0 1 2 5] [3 4]",
            "D": "[0 1 1 4] [2 5]\n[1 2 5 5] [0 3]\n[0 2 0 5] [1 4]\n[1 3 1 5] [0 2]\n[0 2 0 4] [1 5]\n[0 1 5 5] [3 4]"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Which of the following is a hyperparameter?",
        "options": {
            "A": "L1-ratio in elasticnet",
            "B": "Pruning parameter in a decision tree",
            "C": "Learning rate in SGDRegressor",
            "D": "All of these"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following estimators can implement the partial_fit method ?",
        "options": {
            "A": "DecisionTreeClassifier",
            "B": "RandomForestRegressor",
            "C": "LogisticRegressor",
            "D": "SGDRegressor"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\npipe = ([\n    ('scaler', StandardScaler()),\n    ('softmax', _______________)\n])",
        "question": "Which of the following options is True for blank space if I want to train the above pipeline as softmax regression ?",
        "options": {
            "A": "LogisticRegression(solver = 'sag')",
            "B": "LogisticRegression(multi_class = 'ovr')",
            "C": "LogisticRegression(solver = 'lbfgs')",
            "D": "LogisticRegression(multi_class = 'multinomial')"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.metrics import f1_score\ny_true = [1,1,0,1,0,0,1,0,1]\ny_pred = [0,1,0,1,0,1,1,1,1]\nprint(f1_score(y_true,y_pred))",
        "question": "What will be the output?",
        "options": {
            "A": "0.66",
            "B": "0.72",
            "C": "0.80",
            "D": "1.00"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.svm import SVC\nclf = SVC(C=1).fit(X_train, y_train)\nprint(clf.support_)",
        "question": "What will be the output?",
        "options": {
            "A": "It will print number of support vectors",
            "B": "It will print an array of support vectors",
            "C": "It will print an array of probabilities representing distance from decision boundary with each data point.",
            "D": "It will print indices of the support vectors from the training set."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.neighbors import NearestNeighbors\nneigh = NearestNeighbors(n_neighbors=1)\nneigh.fit(X_train)\nprint(neigh.kneighbors(X_test[1:2,:]))\nAssume X_train and X_test are of type numpy.ndarray .",
        "question": "Consider below code which of the following option is true for that",
        "options": {
            "A": "It will print nearest neighbours from the test point",
            "B": "It will print indices of and distances to the neighbouring points (in training set) from test point",
            "C": "It will print indices, distance and nearest training point from the test point",
            "D": "It will throw an error"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Which of the following APIs only supports conjoining transformers and estimators in series (i.e. one after another)?",
        "options": {
            "A": "Pipeline",
            "B": "ColumnTransformer",
            "C": "FeatureUnion",
            "D": "All of these"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following ML task/steps for a regression dataset:\n\n1. Read the data from a file (named 'dataset.csv'). It has 7 columns. The last column is the target variable, all 6 features numerical.\n2. Remove rows which have target values missing.\n3. Fill the missing values in the features by KNN using 3 nearest neighbours.\n4. Split the data into training and test sets. Take randomly the 70% of rows in the training set and the rest of them into the test set.\n5. Train a simple linear regression model, with intercept, on the training set.\n6. Report R2 score on the test set.",
        "question": "Which of the following code snippets correctly accomplishes the above task? Assume necessary imports.",
        "options": {
            "A": "data = pd.read_csv('dataset.csv')\ndata = data.dropna()\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX = X[-y.isna()]\ny = y.dropna()\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.7)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 3)), ('estimator', LinearRegression())])\npipe.fit(X_train,y_train)\nprint(pipe.score(X_test, y_test))",
            "B": "data = pd.read_csv('dataset.csv')\ndata = data.dropna()\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX = X[y.isna()]\ny = y.dropna()\nX_train, X_test, y_train, y_test = train_test_split(X,y, test=0.3)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 3)), ('estimator', LinearRegression())])\npipe.fit(X_train,y_train)\nprint(pipe.score(X_test, y_test))",
            "C": "data = pd.read_csv('dataset.csv')\ndata = data.dropna()\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX = X[-y.isna()]\ny = y[-y.isna()]\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 3)), ('estimator', LinearRegression())])\npipe.fit(X_train,y_train)\nprint(pipe.score(y_test, y_test))",
            "D": "data = pd.read_csv('dataset.csv')\ndata = data.dropna()\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX = X[-y.isna()]\ny = y.dropna()\nX_train, X_test, y_train, y_test = train_test_split(X,y, test=0.2)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 2)), ('estimator', LinearRegression(fit_intercept=False))])\npipe.fit(X_train,X_train)\nprint(pipe.score(X_test, y_test))"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "from sklearn.datasets import make_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nX, y = make_regression(n_samples = 10, n_features = 3)\npoly_transform = PolynomialFeatures(degree=2, interaction_only=True)\nX_trans = poly_transform.fit_transform(X)\nprint(X_trans.shape)",
        "question": "What will be the output of the following code?:",
        "options": {
            "A": "(10, 3)",
            "B": "(10, 7)",
            "C": "(10, 8)",
            "D": "(10, 10)",
            "E": "(10, 11)"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Consider the following statements about SGDClassifier:\n1. It can be used to train a model on large dataset that doesn't fit in main memory\n2. It can emulate a KNN model\n3. It can emulate a decision tree model\n4. It can emulate a perceptron",
        "question": "Choose the correction option(s)",
        "options": {
            "A": "1 and 4",
            "B": "1 and 2",
            "C": "2 and 3",
            "D": "3 and 4"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Consider an image classification task: an image can have dogs, birds and trees. An image can have any combination of these three. The classifier is expected to report all these three for every sample. What kind of classification problem is this?",
        "options": {
            "A": "multi-label and multiclass problem.",
            "B": "multi-label and binary class problem.",
            "C": "multiclass problem.",
            "D": "binary class single label problem."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "model1 = KNeighborsClassifier(n_neighbors=2)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\n\nmodel1.fit(X_train,y_train)\nmodel2.fit(X_train,y_train)",
        "question": "Consider following code snippets, assuming necessary imports. Choose the correct options:",
        "options": {
            "A": "model1 will have smoother decision boundary compared to model model2",
            "B": "model2 will have smoother decision boundary compared to model model1",
            "C": "model1 will have same decision boundary compared as model model2",
            "D": "No comparison can be made between decision boundaries of model1 and model2"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following options is true for the gamma parameter in a non-linear soft margin SVM?",
        "options": {
            "A": "For high values of gamma, the points need to be very close to each other in order to be considered in the same class",
            "B": "For low values of gamma, the points need to be very close to each other in order to be considered in the same class",
            "C": "Gamma doesn't affect the SVM model at all",
            "D": "None of these"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_wine\nX,y = load_wine(as_frame = True, return_X_y = True)\n\ndtc1 = DecisionTreeClassifier(ccp_alpha = 0.0)\ndtc1.fit(X, y)\ndtc2 = DecisionTreeClassifier(ccp_alpha = 0.06)\ndtc2.fit(X, y)\ndtc3 = DecisionTreeClassifier(ccp_alpha = 0.1)\ndtc3.fit(X, y)\ndtc4 = DecisionTreeClassifier(ccp_alpha = 0.03)\ndtc4.fit(X, y)",
        "question": "Which model is likely to overfit the most?",
        "options": {
            "A": "dtc1",
            "B": "dtc2",
            "C": "dtc3",
            "D": "dtc4"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Following is the code to tune the n_estimators parameter of a Bagging Classifier model.\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import BaggingClassifier\n\nparam_grid = [\n  {\n    '__________' : [200, 300, 400, 500, 600]}\n]\npipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                           ('bc', BaggingClassifier())])\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X_train, y_train)",
        "question": "What should the blank space contain?",
        "options": {
            "A": "'n_estimators'",
            "B": "'bc.n_estimators'",
            "C": "'bc__n_estimators'",
            "D": "'bc___n_estimators'",
            "E": "'bc.n_estimators'"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [{'max_depth':range(1, 10, 2)}, {'min_samples_split': range(1, 10, 3)}]\ngs = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 10)\ngs.fit(X,y)",
        "question": "Consider the following code. How many DecisionTreeClassifier models will be trained internally?",
        "options": {
            "A": "20",
            "B": "200",
            "C": "8",
            "D": "150",
            "E": "15",
            "F": "80"
        },
        "correctOption": "F",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.datasets import load_wine\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX,y = load_wine(as_frame = True,\n                return_X_y = True)\nX_train,X_test,y_train,y_test = train_test_split(X,\n                                                  y,\n                                                  test_size = 0.2,\n                                                  random_state = 1)\nclf1 = DecisionTreeClassifier(min_samples_split = 3, min_samples_leaf = 2, random_state = 5)\nclf1.fit(X_train, y_train)\nclf2 = DecisionTreeClassifier(min_samples_split = 6, min_samples_leaf = 4, random_state = 5)\nclf2.fit(X_train, y_train)",
        "question": "What can we say about the depths of the classifiers clf1 and clf2?",
        "options": {
            "A": "depth(clf1) \u2265 depth(clf2)",
            "B": "depth(clf1) \u2264 depth(clf2)",
            "C": "depth(clf1) = depth(clf2)",
            "D": "Insufficient Information"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.tree import DecisionTreeClassifier\nX,y = load_wine(as_frame = True, return_X_y = True)\n\ndtc1 = DecisionTreeClassifier(ccp_alpha = 0.0)\ndtc1.fit(X, y)\ndtc2 = DecisionTreeClassifier(ccp_alpha = 0.03)\ndtc2.fit(X, y)\ndtc3 = DecisionTreeClassifier(ccp_alpha = 0.06)\ndtc3.fit(X, y)\ndtc4 = DecisionTreeClassifier(ccp_alpha = 0.1)\ndtc4.fit(X, y)\n\nd1 = dtc1.get_depth()\nd2 = dtc2.get_depth()\nd3 = dtc3.get_depth()\nd4 = dtc4.get_depth()",
        "question": "What can we say about d1, d2, d3 and d4?",
        "options": {
            "A": "d1 < d2 < d3 < d4",
            "B": "d1 \u2264 d2 \u2264 d3 \u2264 d4",
            "C": "d1 > d2 > d3 > d4",
            "D": "d1 \u2265 d2 \u2265 d3 \u2265 d4"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "Suppose that we have 10000 samples in a dataset. Suppose further we use the K-means algorithm to find clusters in the dataset. Then, the statement that K-Means algorithm always converges with zero inertia (or zero Sums Square Error) for some value of K is",
        "options": {
            "A": "Always True",
            "B": "Always False",
            "C": "True, sometimes"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "km = KMeans(n_clusters=20, init='random', n_init=10, random_state=42)\nkm.fit(X)",
        "question": "Suppose that we use k-means clustering for a dataset having 100 samples. The initial centroids for k clusters are initialized in multiple ways. One such way is shown below\nChoose the correct statements",
        "options": {
            "A": "20 centroids are randomly initialized 10 times",
            "B": "10 centroids are randomly initialized 20 times",
            "C": "20 samples in the dataset are selected as initialization point such that they are at least 10 units away from each other",
            "D": "10 samples in the dataset are selected as initialization point such that they are at least 20 units away from each other"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "We wish to load the wine dataset from sklearn. Which of the following will throw an error?",
        "options": {
            "A": "from sklearn.datasets import load_wine\nX, y = load_wine(load_X_y = True)",
            "B": "from sklearn.datasets import load_wine\ndata = load_wine(load_X_y = True)",
            "C": "from sklearn.datasets import load_wine\ndata = load_wine(return_X_y = True)",
            "D": "from sklearn.datasets import load_wine\nX, y = load_wine(return_X_y = True)"
        },
        "correctOption": [
            "A",
            "B"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following is correct?",
        "options": {
            "A": "SGDClassifier(loss=\"percept\") is stochastic version of a perceptron model",
            "B": "SGDClassifier(loss=\"log_loss\") is stochastic version of a logistic classifier model",
            "C": "SGDClassifier(loss=\"log_loss\") is stochastic version of a SVM model",
            "D": "SGDClassifier(loss=\"hinge\") is stochastic version of a SVM model"
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Which of the following algorithms may get impacted by feature scaling?",
        "options": {
            "A": "LinearRegression",
            "B": "DecisionTree",
            "C": "SVM",
            "D": "BinomialNaiveBayes"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following class(es) is (are) used to instantiate a neural network in Sklearn.",
        "options": {
            "A": "SGDClassifier()",
            "B": "MLPClassifier()",
            "C": "NNClassifier()",
            "D": "MLPRegressor()"
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Following information about X_train is given:\n- Shape of X_train is (100,6)\n- 4 continuous features, 2 categorical features\n- One categorical feature contains 3 categories/unique values\n- Second categorical feature contains 4 categories/unique values\n\nfrom sklearn.preprocessing import OneHotEncoder\nOhe = OneHotEncoder()\nEncoded_X_train = ohe.fit_transform(X_train)\nEncoded_X_train.shape",
        "question": "Which of the following is(are) correct option(s) for above information ?",
        "options": {
            "A": "Encoded_X_train will have more number of columns than X_train",
            "B": "Encoded_X_train will have 11 columns",
            "C": "Encoded_X_train will have more than 11 columns",
            "D": "Encoded_X_train will have more number of rows than X_train"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Which of the following ways can help in feature selection?",
        "options": {
            "A": "Drop a Feature with many missing values",
            "B": "Drop a feature containing data with high standard deviation",
            "C": "Use SelectKBest or SelectKPercentile methods",
            "D": "Drop a feature which has high correlation with target variable"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX,y = load_breast_cancer(as_frame = True,\n                         return_X_y = True)\nX_train,X_test,y_train,y_test = train_test_split(X,\n                                                  y,\n                                                  test_size = 0.2,\n                                                  random_state = 1)\nclf = DecisionTreeClassifier(min_samples_split = 6, min_samples_leaf = 4, random_state = 5)\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))",
        "question": "In which of the following scenarios, the split will NOT be done at node N?",
        "options": {
            "A": "Number of samples at node N = 15. If it is split, it will result in 9 nodes in the left child and 6 nodes in the right child.",
            "B": "Number of samples at node N = 5. If it is split, it will result in 4 nodes in the left child and 2 nodes in the right child.",
            "C": "Number of samples at node N = 7. If it is split, it will result in 4 nodes in the left child and 3 nodes in the right child.",
            "D": "Number of samples at node N = 12. If it is split, it will result in 3 nodes in the left child and 9 nodes in the right child."
        },
        "correctOption": [
            "B",
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "The code given below attempts to find the clusters in the dataset, X using the K-means algorithm.\n\nX,y = make_blobs(n_samples=7,n_features=2,centers=2,random_state=42)\nkm = KMeans(n_clusters=8,init='random',n_init=1,random_state=42)\nkm.fit(X)",
        "question": "Select the true statements about the code upon execution. Assume necessary imports. Note: There are no typos in the code, the argument names passed to the function are all correct",
        "options": {
            "A": "The code attempts to find 2 clusters in the given dataset",
            "B": "The dataset X can be visualized in the Euclidean space",
            "C": "The code raises an error upon execution",
            "D": "The code gets executed without an error upon execution"
        },
        "correctOption": [
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "The following line of code creates a neural network (assume necessary imports)\n\nregr = MLPRegressor(hidden_layer_sizes=(3,5), max_iter=5).fit(X_train, y_train)",
        "question": "Select the correct statements from the following list of statements",
        "options": {
            "A": "The neural network contains 3 hidden layers with 5 neurons in each hidden layer",
            "B": "The neural network contains 5 hidden layers with 3 neurons in each hidden layer",
            "C": "The neural network contains 2 hidden layers with 5 neurons in the second hidden layer",
            "D": "The neural network contains 2 hidden layers with 3 neurons in the first hidden layer",
            "E": "None of the given options are correct"
        },
        "correctOption": [
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples = 100, n_features = 3, n_informative = 2, n_redundant = 1)\n\nparam_grid = [{'max_depth': [2, 3, 4, 5, 6], 'min_samples_split': [2, 3, 4, 5, 6]},\n              {'min_impurity_decrease': [0.2, 0.3, 0.4, 0.5, 0.6], 'ccp_alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]}]\n\ngscv = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 3)\ngscv.fit(X, y)\n\nprint(gscv.best_params_)",
        "question": "How many parameter combinations will be tried by GridSearchCV?",
        "options": null,
        "correctOption": 60,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Select multilabel multiclass classification problems:",
        "options": {
            "A": "There is a collection of photographs. Each photograph can have multiple animals, e.g., cats, dogs and birds. Your model should indicate all the animals which are present.",
            "B": "From appropriate weather data, your model must predict, average temperature and average humidity for next seven days.",
            "C": "Predicting expected price of a second hand car with appropriate features.",
            "D": "None of these."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following options is the correct method to shuffle training data after each epoch in SGDRegressor?",
        "options": {
            "A": "from sklearn.linear_model import SGDRegressor\nlinear_regressor = SGDRegressor(shuffle=True)",
            "B": "from sklearn.preprocessing import SGDRegressor\nlinear_regressor = SGDRegressor(shuffle_per_epoch=True)",
            "C": "from sklearn.SGDRegressor import linear_model\nlinear_regressor = SGDRegressor(learning_rate='constant', eta0=1e-2)",
            "D": "None of these"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following can be used (with appropriate supporting code) to compute training error after each iteration?",
        "options": {
            "A": "SGDRegressor(max_iter=1, warm_start=True, fit_intercept=True, random_state=0, learning_rate='optimal')",
            "B": "SGDRegressor(max_iter=2, warm_start=True, fit_intercept=True, random_state=42, learning_rate='optimal')",
            "C": "SGDRegressor(max_iter=1, warm_start=False, fit_intercept=True, random_state=0, learning_rate='optimal')",
            "D": "SGDRegressor(max_iter=1, warm_start=False, fit_intercept=True, random_state=0, learning_rate='invscaling')",
            "E": "None of these"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1024, n_features=82, n_classes=2, random_state=42)\nestimator = LogisticRegression()\nloocv = LeaveOneOut()\nscores = cross_val_score(estimator, X, y, cv=loocv)",
        "question": "For the given code below in which you use how many models will get trained or what will be the length of scores variable?",
        "options": {
            "A": "1106",
            "B": "82",
            "C": "1024",
            "D": "None of these"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "What is the purpose of k-fold cross-validation?",
        "options": {
            "A": "To split data into training and testing sets.",
            "B": "To tune hyperparameters.",
            "C": "To evaluate model performance on multiple subsets.",
            "D": "To preprocess data."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Consider below statements and choose the correct option\nStatement 1 : `fit` method in scikit-learn helps in learning the parameters of any Classifier or Regressor.\nStatement 2 : The `transform` method in scikit-learn is utilized to apply a transforming function and convert the feature matrix after fitting the Classifier or Regressor.",
        "options": {
            "A": "Statement 1 is True and Statement 2 is False",
            "B": "Statement 2 is True and Statement 1 is False",
            "C": "Both the statements are True",
            "D": "Both the statements are False"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which assumption does Naive Bayes make about the features?",
        "options": {
            "A": "They are independent of each other.",
            "B": "They are always a numerical representation of the text data.",
            "C": "They are linearly related.",
            "D": "They are categorical.",
            "E": "None of these."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nX = np.array([[4.0, 'avocado'],\n              [3.0, 'dragon fruit'],\n              [2.0, 'sapodilla'],\n              [7.0, 'papaya']])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num1', MinMaxScaler(), [0]),\n        ('cat', OneHotEncoder(), [1]),\n        ('num2', StandardScaler(), [0]),\n    ])\n\nX_transformed = preprocessor.fit_transform(X)\nprint(X_transformed[0])",
        "question": "Given the following code snippet that preprocesses a dataset with both continuous and categorical features using sklearn.preprocessing tools, what will be the first row of the X_transformed array after preprocessing?",
        "options": {
            "A": "[0.4, 1, 0, 0, 0]",
            "B": "[0.5, 0, 1, 0, 1.66]",
            "C": "[0.25, 0, 1, 0, -1.66]",
            "D": "[0.4, 0, 0, 1, 0.66]",
            "E": "None of these"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "You're working on a dataset containing customer purchase data, and you want to segment the customers into distinct groups based on their purchasing behavior. Each data point represents a customer and includes features like \u201cTotal Amount Spent\u201d and \u201cNumber of Items Purchased.\u201d Which algorithm is suitable for this scenario?",
        "options": {
            "A": "Linear Regression",
            "B": "Decision Tree",
            "C": "K-means Clustering",
            "D": "Support Vector Machine",
            "E": "None of these"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "You're building an MLPClassifier for a dataset with a large number of features. The goal is to predict whether an online user will purchase a product based on their browsing behavior. You're trying to decide the appropriate number of neurons in the hidden layers of the neural network. Which statement about adjusting the hidden_layer_sizes parameter is correct?",
        "options": {
            "A": "Increasing the number of neurons in hidden layers will always lead to better model performance.",
            "B": "Decreasing the number of neurons in hidden layers reduces the model's capacity to capture complex patterns.",
            "C": "The number of neurons in hidden layers does not significantly affect the model's performance.",
            "D": "Finding the optimal number of neurons is a trial-and-error process and may require experimentation."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "How does agglomerative clustering handle outliers?",
        "options": {
            "A": "It ignores outliers during the clustering process.",
            "B": "It assigns outliers to the nearest cluster.",
            "C": "It creates separate clusters for outliers.",
            "D": "It removes outliers from the dataset before clustering."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "What is agglomerative clustering?",
        "options": {
            "A": "A hierarchical clustering technique that starts with each data point as its cluster and merges the closest clusters iteratively.",
            "B": "A method for partitioning data into a predefined number of clusters.",
            "C": "A clustering algorithm that uses centroids to iteratively assign data points to clusters.",
            "D": "A dimensionality reduction technique that projects data onto a lower-dimensional space."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "What initialization method is best in KMeans to select initial cluster centroids?",
        "options": {
            "A": "Random initialization",
            "B": "K-means++ initialization",
            "C": "Hierarchical agglomerative initialization",
            "D": "Weighted initialization"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "In K-Means clustering, what does the inertia_ attribute represent?",
        "options": {
            "A": "The distance between cluster centroids",
            "B": "The number of clusters formed",
            "C": "Sum of squared distances of samples to their closest cluster center.",
            "D": "The silhouette coefficient"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "You're using a DecisionTreeClassifier from sklearn.tree to build a classification model. Which of the following statements is MOST accurate regarding the parameters and attributes of this classifier?",
        "options": {
            "A": "The depth of the tree always must be less than equal to the max_depth parameter value, while the tree_.max_depth attribute retrieves the depth of the actual tree that was built.",
            "B": "Setting min_samples_split to a value greater than 2 can prevent the tree from splitting on features that have very minimal influence, but this guarantees that all leaf nodes will contain fewer samples than this value.",
            "C": "The criterion='entropy' parameter means that the decision tree will split nodes to maximize information gain, while the tree_.impurity attribute retrieves the impurity of the root node.",
            "D": "If the class_weight parameter is set to 'balanced', the decision tree will always have balanced classes in its leaf nodes."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "You're aiming to optimize an AdaBoostClassifier that uses a DecisionTreeClassifier as its base estimator. You decide to use GridSearchCV from sklearn.model_selection to search for the best hyperparameters. In the given parameter grids for GridSearchCV, parameters n_estimators and learning_rate are meant for the 'AdaBoostClassifier', while the others are for the 'DecisionTreeClassifier'. Which of the following sets of parameters is the MOST comprehensive in testing the capabilities of both the 'AdaBoostClassifier' and its base estimator?",
        "options": {
            "A": "{'AdaBoostClassifier_n_estimators': [50, 100, 150], 'AdaBoostClassifier_learning_rate': [0.01, 0.1, 1]}",
            "B": "{'max_depth': [1, 2, 3], 'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1]}",
            "C": "{'DecisionTreeClassifier_criterion': ['gini', 'entropy'], 'DecisionTreeClassifier_splitter': ['best', 'random'], 'n_estimators': [50, 100], 'learning_rate': [0.1, 1]}",
            "D": "{'estimator__max_depth': [1, 2, 3], 'estimator__criterion': ['gini','entropy'], 'n_estimators': [30, 50], 'learning_rate': [0.05, 0.1, 0.5]}"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nbase_knn = KNeighborsClassifier(n_neighbors=5)\n\nbag_clf = BaggingClassifier(base_knn, n_estimators=50, max_samples=0.5, bootstrap=True, n_jobs=-1)",
        "question": "Which of the following statements is correct?",
        "options": {
            "A": "Bag_clf will throw an error as it only accepts decision tree classifiers as base classifiers.",
            "B": "Each base KNN classifier will be trained on the entire dataset.",
            "C": "The max_samples=0.5 parameter means each base estimator in the ensemble is trained on 50% of the training samples,",
            "D": "The ensemble will use sequential computation due to n_jobs=-1.",
            "E": "None of these"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "You're using a RandomForestClassifier from sklearn for a multi-class classification problem. You want to ensure diversity among the trees to avoid overfitting and increase robustness. Which combination of parameter settings would contribute MOST to achieving this objective?",
        "options": {
            "A": "Setting n_estimators to 10, max_depth to 3, and using criterion='entropy'.",
            "B": "Increasing the value of n_estimators, setting max_features to a value less than the total number of features, and setting bootstrap to False.",
            "C": "Setting n_estimators to a high value, using criterion='gini', and setting max_samples to a value less than the total number of samples.",
            "D": "Setting max_depth to None, min_samples_split to 2, and min_samples_leaf to 1."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "What is the solution for overfitting?",
        "options": {
            "A": "To have less constraints/regularization",
            "B": "To have more constraints/regularization",
            "C": "Delete a significant portion of data",
            "D": "Increase the dataset size."
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Select all the correct options:",
        "options": {
            "A": "The more the SGD iterations, the lesser the fluctuations in training error.",
            "B": "More iterations require more computation time.",
            "C": "The tol (error tolerance) parameter restricts the number of iterations performed.",
            "D": "Training error might not consistently decrease while performing SGD iterations.",
            "E": "None of these"
        },
        "correctOption": [
            "A",
            "B",
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.svm import SVC\nclf = SVC(kernel = ______)\nclf.fit(X, y)",
        "question": "Fill in the missing parameter value in the following estimator that can be used to classify the data",
        "options": {
            "A": "'lasso'",
            "B": "'linear'",
            "C": "'rbf'",
            "D": "'scale'"
        },
        "correctOption": [
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following statements are true?",
        "options": {
            "A": "KNeighborsClassifier with low values of n_neighbors produces complex decision boundaries.",
            "B": "KNeighborsClassifier with low values of n_neighbors produces smooth decision boundaries.",
            "C": "In KNeighborsClassifier the scale of the features (columns) can impact the decision boundaries.",
            "D": "None of these."
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Which of the following options are correct regarding regularization?",
        "options": {
            "A": "It is a technique used to minimize the adjusted loss function and avoid underfitting.",
            "B": "It helps in increasing the bias of the training model.",
            "C": "It determines the rows to be selected as a training dataset.",
            "D": "Elastic net regularization is a combination of L1 and L2 regularization both."
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Which of the following techniques are used in decision trees to make decisions or to measure the quality of a split while training the model?",
        "options": {
            "A": "Entropy",
            "B": "RoC Curve",
            "C": "Cross Entropy",
            "D": "Gini Impurity"
        },
        "correctOption": [
            "A",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "Which of the following approaches is(are) helpful to find a good value for k in k-means clustering algorithm?",
        "options": {
            "A": "Plotting an Elbow curve.",
            "B": "Using classifiers before making clusters.",
            "C": "Plotting Silhouette coefficient for various values of k.",
            "D": "Using k-fold cross validation"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX,y = load_breast_cancer(as_frame = True, return_X_y = True)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)\n\nclf = DecisionTreeClassifier(min_samples_split = 6, min_samples_leaf = 3, random_state = 5)\n\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))",
        "question": "In which of the following scenarios, the split will NOT be made at node N?",
        "options": {
            "A": "10 number of samples at node N. If it is split, it can split such that 2 samples in the left child and 8 samples in the right child.",
            "B": "6 number of samples at node N. If it is split, it can split such that 3 samples in the left child and 3 samples in the right child.",
            "C": "12 number of samples at node N. If it is split, it can split such that 5 samples in the left child and 7 samples in the right child.",
            "D": "4 number of samples at node N. If it is split, it can split such that 3 samples in the left child and 1 samples in the right child."
        },
        "correctOption": [
            "A",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "from sklearn.preprocessing import MaxAbsScaler\na = [[-3.5], [0],[-2.8],[ 2.0], [-1], [-4]]\nmas = MaxAbsScaler()\nscaled_a = mas.fit_transform(a)\nprint(scaled_a.max())",
        "question": "What will be the output of the following code:",
        "options": null,
        "correctOption": 0.5,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "import numpy as np\nx = np.array([[1,1],[10,11],[5,5],[25,18],[-1,-1]])\ny = np.array([0,1,0,1,1]).reshape(-1,1)",
        "question": "What will be the highest accuracy a perceptron model can achieve on this dataset without any feature engineering?",
        "options": null,
        "correctOption": 0.8,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Using the confusion matrix given below. What is the precision score for the label (class) 0?",
        "options": null,
        "correctOption": {
            "min": 0.37,
            "max": 0.38
        },
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764053043/jan2024-1_ogkyc6.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Consider the following Python code snippet and the given dataset that has been stored in the data variable that demonstrates the use of GaussianNB from scikit-learn:",
        "question": "What is the prior probability of the label being \u201cNo\u201d? i.e. p(y = \u201cNo\u201d)",
        "options": null,
        "correctOption": {
            "min": 0.32,
            "max": 0.35
        },
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764053043/jan2024-1_ogkyc6.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.neighbors import KNeighborsClassifier\nX = [[2,3], [5,6], [10, 11], [15,16], [20,21]]\ny = [0, 1, 1, 2, 2]\nknn = KNeighborsClassifier(n_neighbors=3, metric='euclidean', weights='uniform')\nknn.fit(X, y)\nprint(knn.predict([[8,9]]))",
        "question": "What is the output of the following code?",
        "options": null,
        "correctOption": 1,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "from sklearn.datasets import fetch_california_housing, load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nX,y = load_iris(return_X_y= True)\n\npolynomial_transform = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n\ncombined_features = FeatureUnion([('poly', polynomial_transform), ('pca', PCA(n_components=2))])\n\npipeline = Pipeline([('features', combined_features), ('scaler', MinMaxScaler())])\n\nX_transformed = pipeline.fit_transform(X)\n\nprint(X_transformed.shape)",
        "question": "If the shape of X is (150, 4), what will be the number of features in X_transformed?",
        "options": null,
        "correctOption": 36,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "X = np.array([[1,6], [-2,0], [-0.25, 3.5]])\n\npca_transform = PCA(n_components=2)\n\nX_transformed = pca_transform.fit_transform(X)\n\nprint(pca_transform.explained_variance_ratio_[0])",
        "question": "What will be the output of the following code snippet? Assume necessary imports.",
        "options": null,
        "correctOption": 1.0,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.linear_model import Perceptron\n# Sample data\nX = [[0, 0], [0, 1], [1, 0], [1, 1]]\ny = [0, 0, 0, 1]\n\nclf = Perceptron(tol=None, shuffle=False)\nclf.fit(X, y)\n\nprint(clf.predict([[2, 2]]))",
        "question": "What will be the output of the following code snippet?",
        "options": null,
        "correctOption": 1,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following code snippet, assume necessary imports:\ndf=pd.DataFrame(data={\"Name\":['Akash', 'Brajesh', 'Charu', 'Deepak'], \"Maths\":[34,43,66,77], \"English\":[23,45,67,82], \"Hindi\":[53,35,np.nan,\"hi\"]}, index = range(11,15))",
        "question": "Consider the following code snippet:\n`selection = df.loc[13:15, 'Maths':'English']`\nWhich of the following will be equivalent to the given code snippet?",
        "options": {
            "A": "selection = df.iloc[[2,3], [1,2]]",
            "B": "selection = df.iloc[[1,3], [1,2]]",
            "C": "selection = df.iloc[2:4, 1:3]",
            "D": "selection = df.iloc[1:3, 1:2]",
            "E": "None of these"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following code snippet, assume necessary imports:\ndf=pd.DataFrame(data={\"Name\":['Akash', 'Brajesh', 'Charu', 'Deepak'], \"Maths\":[34,43,66,77], \"English\":[23,45,67,82], \"Hindi\":[53,35,np.nan,\"hi\"]}, index = range(11,15))",
        "question": "What is the output of the following code snippet:\n`df.loc[13, 'English'] + df.iloc[0,3]`\nEnter -1, if you think the given statement will generate an error.",
        "options": null,
        "correctOption": 120,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following code snippet, assume necessary imports:\ndf=pd.DataFrame(data={\"Name\":['Akash', 'Brajesh', 'Charu', 'Deepak'], \"Maths\":[34,43,66,77], \"English\":[23,45,67,82], \"Hindi\":[53,35,np.nan,\"hi\"]}, index = range(11,15))",
        "question": "What is the output of the following code snippet:\n`print(df.Hindi.apply(type).nunique())`\nEnter -1, if you think the given statement will generate an error.",
        "options": null,
        "correctOption": 3,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following code snippet, assume necessary imports:\ndf=pd.DataFrame(data={\"Name\":['Akash', 'Brajesh', 'Charu', 'Deepak'], \"Maths\":[34,43,66,77], \"English\":[23,45,67,82], \"Hindi\":[53,35,np.nan,\"hi\"]}, index = range(11,15))",
        "question": "What is the output of the following code snippet:\n`def getPassing(aRow):\n    if aRow['Maths']>=35 and aRow['English']>=35:\n        return True\n    return False\ndf.apply(getPassing).sum()`\nEnter -1, if you think the given statement will generate an error.",
        "options": null,
        "correctOption": -1,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following code snippet, assume necessary imports:\ndf=pd.DataFrame(data={\"Name\":['Akash', 'Brajesh', 'Charu', 'Deepak'], \"Maths\":[34,43,66,77], \"English\":[23,45,67,82], \"Hindi\":[53,35,np.nan,\"hi\"]}, index = range(11,15))",
        "question": "What is the output of the following code snippet:\n`df=df.dropna()`\nChoose correct options from following:",
        "options": {
            "A": "The number of rows will decrease in the dataset.",
            "B": "The number of columns will decrease in the dataset.",
            "C": "The number of columns and number of rows, both, will decrease in the dataset.",
            "D": "There will be no change.",
            "E": "Insufficient information."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Given the following code snippet that preprocesses a dataset with both continuous and categorical features using sklearn.preprocessing tools, what will be the first row of the X_transformed array after preprocessing?\n\npython\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nX = np.array([[2.0, 'apple'],\n              [5.0, 'banana'],\n              [1.0, 'apple'],\n              [4.0, 'cherry']])\n\npreprocessor = ColumnTransformer(\n    transformers=[('num', MinMaxScaler(), [0]),\n                  ('cat', OneHotEncoder(), [1])])\n\nX_transformed = preprocessor.fit_transform(X)\nprint(X_transformed[0])\n",
        "question": "What will be the first row of the X_transformed array after preprocessing?",
        "options": {
            "A": "[0.25, 1, 0, 0]",
            "B": "[0.5, 0, 1, 0]",
            "C": "[0, 1, 0, 0]",
            "D": "[1, 0, 0, 1]"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.DataFrame({\n    'col1': [1, 2, 3, 4, 5],\n    'col2': [10, 20, 30, 40, 50]\n})\n\nss = StandardScaler()\nscaled_data = ss.fit_transform(data)\nprint(ss.var_)\n",
        "question": "What will be the output of the following code?",
        "options": {
            "A": "[4, 400]",
            "B": "[\u221a2, 10\u221a2]",
            "C": "[10, 10000]",
            "D": "[2, 200]"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Imagine you are using a Dummy Regressor with strategy=median. Given the following dataset:\n\n| S.No | X\u2081  | X\u2082  | X\u2083  | Y    |\n|------|-----|-----|-----|------|\n| 1    | 2.3 | 4.5 | 3.2 | 10.5 |\n| 2    | 1.8 | 3.2 | 4.1 | 12.3 |\n| 3    | 2.5 | 4.1 | 2.9 | 11.1 |\n| 4    | 2.0 | 3.8 | 3.5 | 9.8  |\n| 5    | 1.9 | 3.6 | 3.0 | 10.9 |\n| 6    | 2.4 | 4.2 | 3.3 | 11.5 |",
        "question": "What will be the predicted output for an input X = [2.1,3.9,3.2] ?",
        "options": {
            "A": "10.5",
            "B": "11.0",
            "C": "11.5",
            "D": "12.3"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You want to implement an SGDRegressor using scikit-learn with the following specifications:\n- Maximum iterations: 1000\n- Early stopping enabled\n- Learning rate schedule: Adaptive\n- Power tuning for adaptive learning rate: 0.75\n- Tolerance for stopping criteria: 1e-3\n- L2 regularization with strength: 0.01",
        "question": "Which of the following code snippets correctly implements this?",
        "options": {
            "A": "model = SGDRegressor(max_iter=1000, tol=1e-3, early_stopping=True, learning_rate=\"adaptive\", eta0=0.01, power_t=0.75, penalty=\"l2\", alpha=0.01)",
            "B": "model = SGDRegressor(max_iter=1000, tol=1e-3, early_stopping=False, learning_rate=\"constant\", eta0=0.1, power_t=0.5, penalty=\"l1\", alpha=0.05)",
            "C": "model = SGDRegressor(max_iter=500, tol=1e-4, early_stopping=True, learning_rate=\"invscaling\", eta0=0.001, power_t=0.5, penalty=\"elasticnet\", alpha=0.02)",
            "D": "model = SGDRegressor(max_iter=2000, tol=1e-3, early_stopping=True, learning_rate=\"constant\", eta0=0.05, power_t=0.8, penalty=\"l2\", alpha=0.005)"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "What is the primary computational bottleneck of the KNeighborsClassifier?",
        "options": {
            "A": "Training the model.",
            "B": "Choosing the best k value.",
            "C": "Storing and searching through all training samples at prediction time.",
            "D": "Calculating class probabilities."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You are training a Multi-Layer Perceptron (MLP) regressor using Scikit-Learn's MLPRegressor on the California housing dataset. The dataset consists of features such as median income, house age, and population, with the target being median house price.",
        "question": "Which of the following statements is correct about training an MLP regressor on this dataset?",
        "options": {
            "A": "Increasing the number of hidden layers always improves regression accuracy.",
            "B": "Using the ReLU activation function in hidden layers is a good choice for MLP regression.",
            "C": "The output layer should use a softmax activation to predict continuous house prices.",
            "D": "MLPRegressor does not require feature scaling since neural networks automatically normalize input data."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "The following code attempts to implement polynomial regression (degree 4) on the California housing dataset. Please select the appropriate option to complete the missing part correctly.\n\npython\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\n# Load dataset\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the polynomial regression model\nmodel = Pipeline([\n    ('poly', PolynomialFeatures(degree=4)),\n    ('linear', LinearRegression())\n])\n\n# Missing part\n\n# Make predictions\ny_pred = model.predict(X_test)\n",
        "question": "Which of the following correctly fills the missing part while ensuring correct preprocessing and avoiding common pitfalls?",
        "options": {
            "A": "model.fit(X_train, y_train)",
            "B": "model.train(X_train, y_train.reshape(-1, 1))",
            "C": "model.fit(PolynomialFeatures(degree=4).fit_transform(X_train), y_train)",
            "D": "model.fit_transform(X_train, y_train)"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Given the following information, what will be the output of the code snippet?\n\n- continuous: y is an array-like of floats that are not all integers, and is 1D or a column vector.\n- continuous-multioutput: y is a 2D array of floats that are not all integers, and both dimensions are of size > 1.\n- binary: y contains \u2264 2 discrete values and is 1D or a column vector.\n- multiclass: y contains more than two discrete values, is not a sequence of sequences, and is 1D or a column vector.\n- multiclass-multioutput: y is a 2D array that contains more than two discrete values, is not a sequence of sequences, and both dimensions are of size > 1.\n- multilabel-indicator: y is a label indicator matrix, an array of two dimensions with at least two columns, and at most 2 unique values.\n- unknown: y is array-like of object, an array of 3 dimensions, a sequence of sequences, or an array of non-sequence objects.\n\npython\nfrom sklearn.utils.multiclass import type_of_target\nimport numpy as np\n\nprint(type_of_target(['a', 'b', 'a']))\nprint(type_of_target(np.array([['horror', 'fantasy'], ['adventure', 'fantasy'], ['adventure', 'fantasy']])))\nprint(type_of_target(np.array([[2.0, 3.0], [3.0, 2.0]])))\nprint(type_of_target(np.array([[0, 1], [1, 1]])))\n",
        "question": "What will be the output of the code snippet?",
        "options": {
            "A": "'multiclass'\n'continuous-multioutput'\n'unknown'\n'binary'",
            "B": "'multiclass-binary'\n'continuous-multioutput'\n'multiclass-multioutput'\n'multilabel-multioutput'",
            "C": "'binary'\n'multiclass-multioutput'\n'multilabel-indicator'\n'multilabel-indicator'",
            "D": "'binary'\n'multiclass'\n'multioutput'\n'binary'"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following statements about the Naive Bayes algorithm is incorrect?",
        "options": {
            "A": "Naive Bayes assumes that features are conditionally independent given the class label.",
            "B": "Gaussian Naive Bayes is suitable for datasets with continuous features.",
            "C": "Multinomial Naive Bayes is commonly used for text classification tasks.",
            "D": "Naive Bayes always outperforms logistic regression on all datasets."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX = [[0], [1], [2], [3]]\ny = [0, 0, 1, 1]\n\nknn_clf = KNeighborsClassifier(n_neighbors=3, p=1, metric='minkowski')\nknn_clf.fit(X, y)\n\nprint(len(knn_clf.classes_))\nprint(knn_clf.effective_metric_)\n",
        "question": "What will be the output of the following code snippet?",
        "options": {
            "A": "1\n'minkowski'",
            "B": "2\n'manhattan'",
            "C": "3\n'euclidean'",
            "D": "4\n'minkowski'"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "The following code snippet shows the use of two models built on the data X and label y:\n\npython\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\n\nX = np.array([[0, 0], [0.25, 0.25], [0.5, 0.5], [0.75, 0.75], [1, 1]])\ny = np.array([0, 0, 1, 1, 0])\n\nmodel_1 = Pipeline([\n    ('knn', KNeighborsClassifier(n_neighbors=3))\n])\n\nmodel_2 = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('knn', KNeighborsClassifier(n_neighbors=3))\n])\n\nmodel_1.fit(X, y)\nmodel_2.fit(X, y)\n",
        "question": "For the given data, which of the following options is correct?",
        "options": {
            "A": "The approach in model_1 is better than that of model_2",
            "B": "The approach in model_2 is better than that of model_1",
            "C": "The approach of both models will give the same results",
            "D": "This code snippet will throw an error."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "While training a model using AdaBoostClassifier, you observe that the model is overfitting on the training data. Which of the following methods will help mitigate overfitting?",
        "options": {
            "A": "Increase the value of the parameter n_estimators significantly.",
            "B": "Use a larger value for the parameter learning_rate to make the algorithm converge faster.",
            "C": "Remove weak learners from the ensemble manually.",
            "D": "Lower the learning_rate to improve generalization performance."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Which of the following is TRUE about VotingClassifier in Scikit-Learn?",
        "options": {
            "A": "It can only be used with classifiers that output probabilities.",
            "B": "When voting='hard', it chooses the class with the highest sum of predicted probabilities.",
            "C": "When voting='soft', it averages the predicted probabilities and picks the class with the highest probability.",
            "D": "It cannot be used with Pipeline objects."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "You are training a classifier to recognize handwritten digits from the MNIST dataset using Scikit-Learn. The dataset consists of grayscale images (0-9), each flattened into a 1D vector of 784 features.",
        "question": "Which preprocessing step is most essential for improving the performance of an MLPClassifier on the MNIST dataset?",
        "options": {
            "A": "Convert images to grayscale.",
            "B": "Scaling the data using Min-Max Scaling",
            "C": "Apply PCA to reduce feature dimensions.",
            "D": "One-Hot encode the target labels (digits 0-9)"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Consider the following Python code snippet and the given dataset that has been stored in the data variable that demonstrates the use of GaussianNB from scikit-learn:\n\n`data.head() # output shown below`\n\n|    | Outlook  | Temperature | Humidity | Windy | Play Golf |\n|----|----------|-------------|----------|-------|-----------|\n| 0  | Rainy    | Hot         | High     | False | No        |\n| 1  | Overcast | Hot         | High     | False | Yes       |\n| 2  | Sunny    | Cool        | Normal   | False | Yes       |\n| 3  | Sunny    | Cool        | Normal   | True  | No        |\n| 4  | Overcast | Cool        | Normal   | True  | Yes       |\n| 5  | Rainy    | Cool        | Normal   | False | Yes       |\n| 6  | Rainy    | Hot         | High     | True  | No        |\n| 7  | Overcast | Hot         | High     | False | Yes       |\n| 8  | Sunny    | Cool        | Normal   | True  | No        |\n| 9  | Overcast | Cool        | Normal   | True  | Yes       |\n| 10 | Rainy    | Cool        | Normal   | False | Yes       |\n| 11 | Overcast | Hot         | Normal   | False | Yes       |\n\npython\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB\n\nX = data.drop(\"Play Golf\", axis=1)\ny = data[\"Play Golf\"]\n\nX = OneHotEncoder(sparse_output=False).fit_transform(X)\ny = LabelEncoder().fit_transform(y)\n\nestimator = GaussianNB()\nestimator.fit(X,y)\n\nprint(estimator.class_prior_) # gives the priori of labels(y)\n",
        "question": "What is the prior probability of the label being \u201cYes\u201d? i.e. P(y = \u201cYes\u201d)",
        "options": null,
        "correctOption": {
            "min": 0.659,
            "max": 0.68
        },
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Which of the following Pipeline implementations correctly performs Ridge Regression using Stochastic Gradient Descent (SGD) with RandomizedSearchCV for hyperparameter tuning?",
        "options": {
            "A": "python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([('scaler', StandardScaler()),\n                     ('ridge', Ridge(solver='saga'))])\n\nparam_dist = {'ridge__alpha': [0.01, 0.1, 1, 10, 100]}\nsearch = RandomizedSearchCV(estimator = pipeline, cv=5, param_distributions=param_dist)\n",
            "B": "python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([('scaler', StandardScaler()),\n                     ('sgd', SGDRegressor(penalty='l2'))])\n\nparam_dist = {'sgd__alpha': [0.01, 0.1, 1, 10, 100]}\nsearch = RandomizedSearchCV(estimator = pipeline, cv=5, param_distributions=param_dist)\n",
            "C": "python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([('scaler', StandardScaler()),\n                     ('sgd', SGDClassifier(penalty='l2'))])\n\nparam_dist = {'sgd__alpha': [0.01, 0.1, 1, 10, 100]}\nsearch = RandomizedSearchCV(estimator = pipeline, cv=5, param_distributions=param_dist)\n",
            "D": "python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([('scaler', StandardScaler()),\n                     ('ridge', RidgeCV(alphas=[0.01, 0.1, 1, 10, 100]))])\n\nparam_dist = {'ridge__alpha': [0.01, 0.1, 1, 10, 100]}\nsearch = RandomizedSearchCV(estimator = pipeline, cv=5, param_distributions=param_dist)\n"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Consider the following definition of the SVC from the sklearn documentation:\n\n`class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)`",
        "question": "Which of the following options are True:",
        "options": {
            "A": "The kernel parameter can take values such as 'linear' and 'sigmoid'.",
            "B": "The regularization parameter C must be between -1 and 1.",
            "C": "The degree parameter is only considered when the kernel is set to 'poly'.",
            "D": "A large value of the regularization parameter C will cause underfitting and result in smoother decision boundaries."
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Which of the following techniques can be used for hyperparameter tuning in Ridge Regression? Select all correct options.",
        "options": {
            "A": "GridSearchCV",
            "B": "ElasticNetCV",
            "C": "RandomizedSearchCV",
            "D": "Mini batch gradient descent"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Which of the following statements are true regarding the RandomForestClassifier?",
        "options": {
            "A": "The randomness in the model can be from having bootstrap=\"true\" or having max_features < n_features.",
            "B": "oob_score can be used to get a generalization score when bootstrap=\"true\"",
            "C": "The class_weight value can be used to associate weights with classes",
            "D": "When bootstrap=\"false\", the parameter max_samples can be used to decide the number of samples to be drawn to train each base estimator"
        },
        "correctOption": [
            "A",
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "In top-down (divisive) hierarchical clustering, clusters are recursively split until each data point is its own cluster. The choice of a distance metric plays a crucial role in determining the quality of clustering.",
        "question": "Which of the following distance (similarity) metrics are commonly used in hierarchical clustering?",
        "options": {
            "A": "Euclidean Distance",
            "B": "Manhattan Distance",
            "C": "Jaccard Similarity",
            "D": "Cosine Distance"
        },
        "correctOption": [
            "A",
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You are training a Multi-Layer Perceptron (MLP) using Scikit-Learn's MLPClassifier for a binary classification task. However, some implementations contain syntax errors. Which of the following implementations are syntactically correct? Select all that apply.\n\npython\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = make_classification(n_samples=1500, n_features=30, random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = #fill in the blank\n\nmodel.fit(X_train, y_train)\n",
        "question": "Which of the following implementations are syntactically correct? Select all that apply.",
        "options": {
            "A": "MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', early_stopping=True)",
            "B": "MLPClassifier(hidden_layer_sizes=(64,), activation='logistic', solver='adam', max_iter=\"300\")",
            "C": "MLPClassifier(hidden_layer_sizes=(50, 50), activation='relu', solver='lbfgs', max_iter=200)",
            "D": "MLPClassifier(hidden_layer_sizes=[[128], [64]], activation='relu', solver='adam', early_stopping=True)"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following statements are true with regard to logistic regression?",
        "options": {
            "A": "It is a powerful method to solve regression tasks such as income prediction, house price prediction, and sales prediction.",
            "B": "The predict_proba(X) method can be used to obtain the predicted probability of each class by the model for the data X.",
            "C": "The penalty parameter is used to decide the type of regularization used.",
            "D": "Setting the max_iter parameter to 100 ensures that the model takes exactly 100 iterations to finish training."
        },
        "correctOption": [
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following code snippet:\n\npython\nfrom sklearn.datasets import fetch_california_housing, load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nX,y = load_iris(return_X_y= True)\n\npolynomial_transform = PolynomialFeatures(degree=3, \n                                          interaction_only=False, \n                                          include_bias=False)\n\ncombined_features = FeatureUnion([('poly', polynomial_transform),\n                                  ('pca', PCA(n_components=2))])\n\npipeline = Pipeline([('features', combined_features),\n                     ('scaler', MinMaxScaler())])\n\nX_transformed = pipeline.fit_transform(X)\n\nprint(X_transformed.shape)\n",
        "question": "If the shape of X is (150, 4), what will be the number of features in X_transformed?",
        "options": null,
        "correctOption": 36,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Use the confusion matrix given below. What is the recall score for the label (class) 1?",
        "question": "Use the confusion matrix given below. What is the recall score for the label (class) 1?",
        "options": null,
        "correctOption": 0.1,
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764053452/jan2025-1_gbnihw.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following code snippet, assume necessary imports:\n\npython\ndf=pd.DataFrame(data={'Name':['Akash','Brajesh','Charu','Deepak'],\n                     'Maths':[34,43,56,77],\n                     'History':[23,45,67,82],\n                     'Hindi':[53,35,np.nan,'bye']},\n                index = range(21,25))\n\n\nBased on the above data, answer the given subquestions.",
        "question": "Consider the following code snippet:\n\n`selection = df.loc[23:25, 'Maths':'History']`\n\nWhich of the following will be equivalent to the above code snippet?",
        "options": {
            "A": "selection = df.iloc[[2,3], [1,2]]",
            "B": "selection = df.iloc[[1,3], [1,2]]",
            "C": "selection = df.iloc[2:4, 1:3]",
            "D": "selection = df.iloc[1:3, 1:2]",
            "E": "None of these"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following code snippet, assume necessary imports:\n\npython\ndf=pd.DataFrame(data={'Name':['Akash','Brajesh','Charu','Deepak'],\n                     'Maths':[34,43,56,77],\n                     'History':[23,45,67,82],\n                     'Hindi':[53,35,np.nan,'bye']},\n                index = range(21,25))\n\n\nBased on the above data, answer the given subquestions.",
        "question": "What is the output of the following code snippet:\n\n`df.loc[23, 'History'] + df.iloc[0,3]`\n\nEnter -1, if you think the above statement will generate an error.",
        "options": null,
        "correctOption": 120,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following code snippet, assume necessary imports:\n\npython\ndf=pd.DataFrame(data={'Name':['Akash','Brajesh','Charu','Deepak'],\n                     'Maths':[34,43,56,77],\n                     'History':[23,45,67,82],\n                     'Hindi':[53,35,np.nan,'bye']},\n                index = range(21,25))\n\n\nBased on the above data, answer the given subquestions.",
        "question": "What is the output of the following code snippet:\n\npython\ndef getPassing(aRow):\n    if aRow['Maths']>40 and aRow['History']>=40:\n        return True\n    return False\n\ndf.apply(getPassing).sum()\n\n\nEnter -1, if you think the above statement will generate an error.",
        "options": null,
        "correctOption": -1,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider following common data:\n\n`print(\"Classification Report:\\n\", classification_report(y_test, y_pred))`\n\nClassification Report:\n\n|           | precision | recall | f1-score | support |\n|-----------|-----------|--------|----------|---------|\n| 0         | 0.86      | 0.67   | 0.75     | 9       |\n| 1         | 0.97      | 0.99   | 0.98     | 105     |\n|           |           |        |          |         |\n| accuracy  |           |        | 0.96     | 114     |\n| macro avg | 0.91      | 0.83   | 0.87     | 114     |\n| weighted avg| 0.96      | 0.96   | 0.96     | 114     |\n\nConsider the above classification report on a model where y_test contains the original labels and y_pred contains the predicted labels.\n\nBased on the above data, answer the given subquestions.",
        "question": "Consider the following assertion & reason and select the correct option:\n\nAssertion: This is a good model as we see a very high accuracy of 96%\nReason: In the case of imbalanced datasets, accuracy might not be the best metric for evaluating a model",
        "options": {
            "A": "Both the assertion and reason are correct, and the reason is a correct explanation of the assertion",
            "B": "Both the assertion and reason are correct, but the reason is not a correct explanation of the assertion",
            "C": "The assertion is correct, but the reason is incorrect.",
            "D": "The assertion is incorrect, but the reason is correct."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider following common data:\n\n`print(\"Classification Report:\\n\", classification_report(y_test, y_pred))`\n\nClassification Report:\n\n|           | precision | recall | f1-score | support |\n|-----------|-----------|--------|----------|---------|\n| 0         | 0.86      | 0.67   | 0.75     | 9       |\n| 1         | 0.97      | 0.99   | 0.98     | 105     |\n|           |           |        |          |         |\n| accuracy  |           |        | 0.96     | 114     |\n| macro avg | 0.91      | 0.83   | 0.87     | 114     |\n| weighted avg| 0.96      | 0.96   | 0.96     | 114     |\n\nConsider the above classification report on a model where y_test contains the original labels and y_pred contains the predicted labels.\n\nBased on the above data, answer the given subquestions.",
        "question": "How many points belong to class 1 in the original dataset?",
        "options": null,
        "correctOption": 105,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Jan 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider following common data:\n\n`print(\"Classification Report:\\n\", classification_report(y_test, y_pred))`\n\nClassification Report:\n\n|           | precision | recall | f1-score | support |\n|-----------|-----------|--------|----------|---------|\n| 0         | 0.86      | 0.67   | 0.75     | 9       |\n| 1         | 0.97      | 0.99   | 0.98     | 105     |\n|           |           |        |          |         |\n| accuracy  |           |        | 0.96     | 114     |\n| macro avg | 0.91      | 0.83   | 0.87     | 114     |\n| weighted avg| 0.96      | 0.96   | 0.96     | 114     |\n\nConsider the above classification report on a model where y_test contains the original labels and y_pred contains the predicted labels.\n\nBased on the above data, answer the given subquestions.",
        "question": "What is the output obtained when the f1_score() function from sklearn is used on y_test and y_pred ?",
        "options": null,
        "correctOption": 0.98,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "from sklearn.preprocessing import MaxAbsScaler\na = [[-3],[0],[-2],[2],[-1],[-4]]\nmas = MaxAbsScaler()\nscaled_a = mas.fit_transform(a)\nprint(scaled_a.max())",
        "question": "What will be the output of the following code:",
        "options": null,
        "correctOption": 0.5,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Given a y_train list which consists of pizza's ordered by the customers in a shop.\ny_train = [['regular', 'veg'],\n['medium', 'veg'],\n['regular', 'non-veg'],\n['medium', 'non-veg']]\nMultiLabelBinarizer from sklearn library has been used to convert the y_train into numbers, so which of the following option matches with the output using the following code ?\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer(classes=['regular','medium', 'veg', 'non-veg'])\nprint(mlb.fit_transform(y_train))",
        "question": "Given a y_train list which consists of pizza's ordered by the customers in a shop.\ny_train = [['regular', 'veg'],\n['medium', 'veg'],\n['regular', 'non-veg'],\n['medium', 'non-veg']]\nMultiLabelBinarizer from sklearn library has been used to convert the y_train into numbers, so which of the following option matches with the output using the following code ?\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer(classes=['regular','medium', 'veg', 'non-veg'])\nprint(mlb.fit_transform(y_train))",
        "options": {
            "A": "[[1 0 1 0 0 1 0 1 0 1 1 0]]",
            "B": "[[1 0 1 0], [0 1 1 0], [1 0 0 1], [0 1 0 1]]",
            "C": "[[1 0 1 0 1 0], [0 0 1 0 1 1 1 0]]",
            "D": "[[1 0], [1 0], [0 1], [1 0], [1 0], [0 1], [0 1], [0 1]]"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "You're building a machine learning pipeline to preprocess data and train a model on a classification task. You decide to use a pipeline that includes data preprocessing and a support vector machine (SVM) classifier.\nThe following code snippet demonstrates the pipeline creation and usage:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Simulated data (features: X, target: y)\nX = np.array([[2, 3], [5, 7], [8, 10]])\ny = np.array([0, 1, 0])\n\n# Create a pipeline with StandardScaler and SVM classifier\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm', SVC())\n])\n\n# Fit the pipeline on training data\npipeline.fit(X, y)\n\n# Make predictions using the trained pipeline\npredictions = pipeline.predict(X)",
        "question": "What is the purpose of using the pipeline in this code snippet?",
        "options": {
            "A": "The pipeline combines multiple models for better model performance.",
            "B": "The pipeline allows for simultaneous training of the scaler and classifier.",
            "C": "The pipeline simplifies the code by encapsulating preprocessing and modeling steps.",
            "D": "The pipeline ensures that only linear SVM can be used for this classification task."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "You've developed a binary classification model to predict whether an email is spam or not. You want to evaluate the model's performance using appropriate metrics. The following options represent different evaluation metrics. Choose the one that is most suitable for assessing the model's performance in this scenario.",
        "options": {
            "A": "R-squared (R\u00b2)",
            "B": "Mean Absolute Error (MAE)",
            "C": "F1-score",
            "D": "Root Mean Squared Error (RMSE)"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You are building a sentiment analysis model using scikit-learn's SGDClassifier to classify movie reviews as positive or negative. The dataset is quite large, and you're dealing with a high-dimensional feature space. You want to fine-tune the hyperparameters of the classifier to achieve better convergence and classification performance. Here's how you're setting up the SGDClassifier:\n\nfrom sklearn.linear_model import SGDClassifier\nclassifier = SGDClassifier(loss='hinge', alpha=0.0001, max_iter=1000, tol=1e-3, power_t=0.5)",
        "question": "In the context of the given code and scenario, what does the power_t parameter value of 0.5 influence during the training process?",
        "options": {
            "A": "It determines the degree of L2 regularization applied to the model's weights.",
            "B": "It controls the decay rate of the learning rate during each iteration.",
            "C": "It sets the threshold for early stopping based on the loss function improvement.",
            "D": "It adjusts the aggressiveness of stochastic gradient updates for faster convergence."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "corpus = ['An overfitted model is a mathematical model that contains more parameters than can be justified by the data.']\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nvectors = vectorizer.fit_transform(corpus)\nprint(vectors.shape)",
        "question": "What is the output of the following code?",
        "options": {
            "A": "(16,1)",
            "B": "(8,2)",
            "C": "(4,4)",
            "D": "(1,16)"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "When using machine learning algorithms, setting the random state parameter to a specific value provides which of the following benefits?",
        "options": {
            "A": "It ensures that the model will always produce the same predictions for any input data.",
            "B": "It guarantees that the model will converge to the global optimum during training.",
            "C": "It prevents overfitting by adding randomness to the model's predictions.",
            "D": "It allows for reproducibility, ensuring consistent results across different runs."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following code involving the use of LabelEncoder from sklearn.preprocessing:\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndata = [\"apple\", \"orange\", \"banana\", \"apple\", \"grape\", \"orange\", \"grape\"]\n\nencoder = LabelEncoder()\nencoded_data = encoder.fit_transform(data)\n\nprint(encoded_data)",
        "question": "Which of the following statements is true based on the code?",
        "options": {
            "A": "After calling encoder.transform([\"pineapple\"]), an error will be raised.",
            "B": "LabelEncoder assigns smaller integer values to labels appearing earlier in the dataset.",
            "C": "If the encoded_data contains the integer 3, it corresponds to the label \"apple\".",
            "D": "LabelEncoder encodes the data based on the alphabetical order of the labels."
        },
        "correctOption": [
            "A",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following statements are true?",
        "options": {
            "A": "KNeighborsClassifier with high values of n_neighbors produces complex decision boundaries.",
            "B": "KNeighborsClassifier with high values of n_neighbors produces smooth decision boundaries.",
            "C": "In KNeighborsClassifier the scale of the features(columns) can impact the decision boundaries.",
            "D": "None of these"
        },
        "correctOption": [
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Which of the following option(s) are correct regarding regularization?",
        "options": {
            "A": "Regularization is the error given by any model while predicting the values for the test set.",
            "B": "It helps in decreasing the bias of the training model.",
            "C": "Compare to without regularized model regularization decreases the variance of the training model",
            "D": "Predictions made with the Ridge Regression are less sensitive to weights(coefficients) than the Linear Regression."
        },
        "correctOption": [
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following sklearn code snippet that employs a Pipeline for data preprocessing:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, Binarizer\n\ndata = [[1, 5], [5, 15], [0, 10], [1, 15]]\n\npipeline = Pipeline([('scaler', MinMaxScaler()),\n                     ('binarize', Binarizer(threshold=0.1))])\n\ntransformed_data = pipeline.fit_transform(data)\n\nWhat will be the output of the following code?\n\nprint(transformed_data[0])",
        "question": "What will be the output of the following code?\n\nprint(transformed_data[0])",
        "options": {
            "A": "[0. 0.]",
            "B": "[1. 0.]",
            "C": "[1. 1.]",
            "D": "[0. 1.]"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "Consider the following code snippet that uses PCA from sklearn.decomposition for dimensionality reduction on a dataset with 10 features:\n\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\nX = np.random.rand(100, 10)\n\npca = PCA(n_components=4)\nX_pca = pca.fit_transform(X)\n\nexplained_variance = np.sum(pca.explained_variance_ratio_)",
        "question": "After executing this code, which of the following statements is true regarding the explained_variance?",
        "options": {
            "A": "The explained_variance represents the combined explained variance of all 10 principal components.",
            "B": "To get a dataset with more than 90% of the original variance, you can set n_components to 0.9 when initializing PCA.",
            "C": "The X_pca dataset contains the original 10 features but with reduced dimensionality.",
            "D": "The principal components in X_pca are based on the most significant original features."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You've built a linear regression model to predict the salary of employees based on their years of experience and education level. The model's coefficients for the features are as follows:\n\n- Coefficient for Years of Experience: 5000\n- Coefficient for Education Level: 3000",
        "question": "What does the coefficient for \u201cYears of Experience\u201d (5000) represent in this context?",
        "options": {
            "A": "For each additional year of experience, an employee's salary is expected to increase by 5000.",
            "B": "For each additional year of experience, an employee's salary is expected to decrease by 5000.",
            "C": "Education level has a stronger impact on salary than years of experience.",
            "D": "The coefficient doesn't have any meaningful interpretation in this scenario."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "You're working on a dataset that includes data points for a single feature and a target variable. You decide to use polynomial regression with a degree of 3 to capture potential cubic relationships. The following code snippet demonstrates the process:\n\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n\n# Simulated data (feature: X)\nX = np.array([1, 2, 3, 4, 5])\n\n# Reshape the features\nX = X.reshape(-1, 1)\n\n# Transform features into polynomial features\npoly = PolynomialFeatures(degree=3)\nX_poly = poly.fit_transform(X)",
        "question": "What will be the shape of the X_poly matrix after transforming the feature 'X' into polynomial features of degree 3?",
        "options": {
            "A": "(5, 1)",
            "B": "(5, 3)",
            "C": "(5, 4)",
            "D": "(5, 6)"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "data = [[1, 3],\n        [2, 4]]\n\nfrom sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree=3,interaction_only=False)\nprint(pf.fit_transform(data))",
        "question": "Choose the correct output of the following code?",
        "options": {
            "A": "[[1, 2, 3, 2, 4], [1, 2, 4, 6, 8]]",
            "B": "[[1, 1, 3, 3], [1, 2, 4, 8]]",
            "C": "[[1, 1, 3, 1, 3, 9, 1, 3, 9, 27], [1, 2, 4, 4, 8, 16, 8, 16, 32, 64]]",
            "D": "[[1, 1, 2, 2, 3, 3, 4, 4], [1, 1, 4, 4, 9, 9, 16, 16]]"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Upon successfully training a LogisticRegression model from sklearn.linear_model on a binary classification problem, you decide to inspect some of its attributes. Which of the following attributes would provide you with the estimated probabilities that each sample belongs to a particular class?",
        "options": {
            "A": "model.classes_",
            "B": "model.decision_function()",
            "C": "model.predict_proba()",
            "D": "model.n_iter_"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Imagine you're training a Perceptron using sklearn with the following code:\n\nfrom sklearn.linear_model import Perceptron\n\nX = [[0, 0.5], [1, 1.5], [1, 2], [2, 3]]\ny = [-1, -1, 1, 1]\n\nclf = Perceptron(tol=None, shuffle=True, random_state=42)\nclf.fit(X, y)\n\niterations = clf.n_iter_",
        "question": "Given the linearly separable nature of the data, how many iterations would it most likely take for the perceptron to converge? What will be the value of iterations?",
        "options": {
            "A": "iterations = 1",
            "B": "iterations = 10",
            "C": "iterations value can vary since the data is being shuffled in each epoch.",
            "D": "iterations = 5"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "You're working on a multi-class classification task with high-dimensional data. You decide to use the RidgeClassifier algorithm for its regularization properties. However, you're concerned about selecting an appropriate value for the regularization parameter alpha. Which of the following statements about choosing the alpha value for 'RidgeClassifier' is correct?",
        "options": {
            "A": "A higher alpha value results in stronger regularization, leading to larger feature coefficients.",
            "B": "The alpha value has no impact on the regularization strength in 'RidgeClassifier'.",
            "C": "Cross-validation can be used to find the optimal alpha value.",
            "D": "'RidgeClassifier' automatically determines the alpha value based on the data distribution."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "You're applying the K-means clustering algorithm to a dataset with a large number of data points. You decide to use the K-means++ initialization method for better convergence and clustering quality. What is the primary advantage of using K-means++ initialization over random initialization?",
        "options": {
            "A": "K-means++ initialization guarantees finding the global optimum of the clustering solution.",
            "B": "K-means++ initialization speeds up the convergence of the algorithm.",
            "C": "K-means++ initialization reduces the number of clusters needed for accurate results.",
            "D": "K-means++ initialization helps avoid local optima during clustering."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Suppose you are working on a classification task involving handwritten digit recognition using scikit-learn's MLPClassifier. You have a large dataset of digit images and want to train a neural network for this task. However, you're concerned about overfitting and want to make sure the training process stops at the right time to avoid this issue. In the context of training an MLPClassifier for handwritten digit recognition, how does the early stopping parameter help prevent overfitting?",
        "options": {
            "A": "It terminates the training process as soon as the specified number of hidden layers are trained.",
            "B": "It automatically decreases the learning rate during training to slow down the optimization process.",
            "C": "It pauses the training when the model's performance on a validation set stops improving.",
            "D": "It enforces a maximum limit on the number of epochs during the training process."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You're training a multi-layer perceptron (MLP) classifier on a dataset for a multi-class classification task. The following code snippet demonstrates the process using the 'MLPClassifier' from scikit-learn:\n\nfrom sklearn.neural_network import MLPClassifier\nimport numpy as np\n\n# Simulated data (features: X, target: y)\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 2])\n\n# Create an MLPClassifier with a specified maximum number of iterations\nmodel = MLPClassifier(max_iter=100, random_state=42)\n\n# Fit the model on the training data\nmodel.fit(X, y)\n\n# Get the number of iterations used in training\niterations_used = model.n_iter_",
        "question": "What does the parameter max_iter=100 in the MLPClassifier signify?",
        "options": {
            "A": "It specifies the maximum number of features to be used during training.",
            "B": "It determines the maximum number of neurons in the hidden layers.",
            "C": "It sets the maximum number of iterations for the training process.",
            "D": "It controls the maximum number of epochs for the training process."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "You're working on a multi-class classification task using the 'MLPClassifier' from scikit-learn. The dataset contains features with varying scales, and you're considering whether to scale the features before training the model. How might feature scaling impact the prediction accuracy of the 'MLPClassifier'?",
        "options": {
            "A": "Feature scaling has no effect on the accuracy of 'MLPClassifier' predictions.",
            "B": "Scaling features is only relevant if the dataset contains categorical features.",
            "C": "Feature scaling can lead to overfitting and decreased prediction accuracy.",
            "D": "Scaling features can help the model converge faster and improve prediction accuracy."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "For LinearRegression with equation Y= W\u2080X\u2080 + W\u2081X\u2081 + W\u2082X\u2082 + \u03f5\nand given that W\u2082 = (6/5) * W\u2081 and \u03f5 = 0. What will be the value of the W\u2081 for the below code? (Write 3 digits after the decimal)\nWhere X\u2081 and X\u2082 are column1 and column2 respectively and W\u2081 and W\u2082 are weights associated to the respected columns while fitting\n\nfrom sklearn.linear_model import LinearRegression\nX_train = [[0,0],[2,2.4],[4,4.8],[6,7.2]]\ny_train = [0,1,2,3]\nreg = LinearRegression(fit_intercept=False) #intercept = 0\nreg.fit(X_train,y_train)\nprint(reg.coef_[0])",
        "question": "For LinearRegression with equation Y= W\u2080X\u2080 + W\u2081X\u2081 + W\u2082X\u2082 + \u03f5 and given that W\u2082 = (6/5) * W\u2081 and \u03f5 = 0. What will be the value of the W\u2081 for the below code? (Write 3 digits after the decimal)",
        "options": null,
        "correctOption": {
            "min": 0.2039,
            "max": 0.2052
        },
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Using the confusion matrix given below. What is the recall score for the label (class) 2? (upto 3 digits after the decimal)",
        "options": null,
        "correctOption": {
            "min": 0.283,
            "max": 0.291
        },
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764053618/may2023-1_fqmsba.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.neighbors import KNeighborsClassifier\nX = [[2,3], [5,6], [10, 11], [15,16], [20,21]]\ny = [2, 2, 0, 1, 0]\nknn = KNeighborsClassifier(n_neighbors=3, metric='euclidean', weights='uniform')\nknn.fit(X, y)\nprint(knn.predict([[8,9]]))",
        "question": "What is the output of the following code?",
        "options": null,
        "correctOption": 2,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Consider the following code snippet that employs LogisticRegression from sklearn on a feature matrix X and corresponding label vector y:\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(C=0.8, multi_class='multinomial', max_iter=1000)\nmodel.fit(X, y)",
        "question": "Given the code above, which of the following statements is true?",
        "options": {
            "A": "The logistic regression model is set up for binary classification.",
            "B": "The model does not use any regularization because the parameter C is set.",
            "C": "The model has been specifically set up to handle a multi-class classification problem using a softmax regression approach.",
            "D": "The model will iterate over the data a maximum of 1000 times, irrespective of convergence."
        },
        "correctOption": [
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following estimators can implement the partial_fit method ?",
        "options": {
            "A": "MultinomialNB",
            "B": "RandomForestRegressor",
            "C": "MiniBatchKMeans",
            "D": "LogisticRegressor"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.svm import SVC\nclf = SVC(kernel = _____)\nclf.fit(X, y)",
        "question": "Fill in the missing parameter value in the following estimator that can be used to classify the data",
        "options": {
            "A": "'poly'",
            "B": "'lasso'",
            "C": "'scale'",
            "D": "'sigmoid'"
        },
        "correctOption": [
            "A",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Consider the following code snippet where two decision trees are trained on the same dataset:\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_digits\n\ndata = load_digits()\nX, y = data.data, data.target\n\ntree_1 = DecisionTreeClassifier(splitter='best', max_leaf_nodes=5)\ntree_1.fit(X, y)\n\ntree_2 = DecisionTreeClassifier(splitter='random', max_leaf_nodes=None)\ntree_2.fit(X, y)",
        "question": "Given the configurations of tree_1 and tree_2, which decision tree is more likely to overfit the training data?",
        "options": {
            "A": "tree_1",
            "B": "tree_2"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Given the following code using BaggingClassifier with KNeighborsClassifier as the base estimator:\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nbase_knn = KNeighborsClassifier(n_neighbors=3, weights='distance')\nbag_clf = BaggingClassifier(base_knn, n_estimators=30, max_samples=100, bootstrap=False, random_state=42)",
        "question": "Which of the following statements is correct?",
        "options": {
            "A": "The ensemble uses bootstrapping to generate samples for each base classifier.",
            "B": "Each base KNN classifier will be trained on a random subset of exactly 100 samples.",
            "C": "Due to weights='distance', each base KNN classifier will treat all neighbors equally in terms of voting power.",
            "D": "The ensemble will consist of 3 base KNN classifiers."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Suppose you are training a RandomForestClassifier from sklearn for a binary classification problem. You're particularly interested in reducing overfitting and enhancing the model's generalization to unseen data. Which combination of parameter settings would be the MOST effective in achieving this goal?",
        "options": {
            "A": "Setting n_estimators to a high value (e.g., 1000) and max_depth to None.",
            "B": "Using criterion='gini', setting min_samples_split to 2, and min_samples_leaf to 1.",
            "C": "Using criterion='entropy' with oob_score=True and max_samples=0.5.",
            "D": "Setting max_features to 'sqrt', bootstrap to True, and increasing the value of min_samples_leaf."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Consider training a classification model using the DecisionTreeClassifier from sklearn.tree. Which of the following statements about its parameters and attributes is MOST accurate?",
        "options": {
            "A": "The ccp_alpha parameter determines the complexity of the tree, with higher values resulting in simpler trees. The tree_.node_count attribute gives the total number of nodes in the tree.",
            "B": "If splitter='best', the classifier guarantees that the resulting tree will have the highest possible accuracy on the training set.",
            "C": "The criterion='entropy' parameter means that the decision tree will split nodes to maximize information gain, while the tree_.impurity attribute retrieves the impurity of the root node.",
            "D": "If the class_weight parameter is set to 'balanced', the decision tree will always have balanced classes in its leaf nodes."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "You're aiming to optimize an AdaBoostClassifier that uses a DecisionTreeClassifier as its base estimator. You decide to use GridSearchCV from sklearn.model_selection to search for the best hyperparameters. In the given parameter grids for GridSearchCV, parameters n_estimator and learning_rate are meant for the 'AdaBoostClassifier', while the others are for the 'DecisionTreeClassifier'. Which of the following sets of parameters is the MOST comprehensive in testing the capabilities of both the 'AdaBoostClassifier' and its base estimator?",
        "options": {
            "A": "{'AdaBoostClassifier_n_estimators': [50, 100, 150], 'AdaBoostClassifier_learning_rate': [0.01, 0.1, 1]}",
            "B": "{'max_depth': [1, 2, 3], 'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1]}",
            "C": "{'DecisionTreeClassifier_criterion': ['gini', 'entropy'], 'DecisionTreeClassifier_splitter': ['best', 'random'], 'n_estimators': [50, 100], 'learning_rate': [0.1, 1]}",
            "D": "{'base_estimator__max_depth': [1, 2, 3], 'base_estimator__criterion': ['gini', 'entropy'], 'n_estimators': [30, 50], 'learning_rate': [0.05, 0.1, 0.5]}"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "You're evaluating the results of a clustering algorithm and you calculate the silhouette score for the clustering solution. The obtained silhouette score is -0.15. What can you infer from this silhouette score?",
        "options": {
            "A": "The clustering solution has well-defined and distinct clusters.",
            "B": "The silhouette score indicates a random distribution of data points across clusters.",
            "C": "The data points are equally spaced across clusters.",
            "D": "The clustering solution is flawed and the data points might be assigned to incorrect clusters."
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following common data and answer the subquestion:\ndf=pd.DataFrame(data={'Name':['Akash','Brajesh','Charu','Deepak'],\n\"Maths\":[34,43,56,77],\n\"History\":[23,45,67,82],\n\"Hindi\":[53,35,np.nan,\"bye\"]},\nindex = range(21,25))",
        "question": "Which of the following will be equivalent to the above code snippet?\nselection = df.loc[23:24, 'Maths':'History']",
        "options": {
            "A": "selection = df.iloc[[2,3], [1,2]]",
            "B": "selection = df.iloc[[1,3], [1,2]]",
            "C": "selection = df.iloc[2:4, 1:3]",
            "D": "selection = df.iloc[1:3, 1:2]",
            "E": "None of these"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following common data and answer the subquestion:\ndf=pd.DataFrame(data={'Name':['Akash','Brajesh','Charu','Deepak'],\n\"Maths\":[34,43,56,77],\n\"History\":[23,45,67,82],\n\"Hindi\":[53,35,np.nan,\"bye\"]},\nindex = range(21,25))",
        "question": "What is the output of the following code snippet:\ndf.loc[23, 'History'] + df.iloc[0,3]",
        "options": null,
        "correctOption": 120,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following common data and answer the subquestion:\ndf=pd.DataFrame(data={'Name':['Akash','Brajesh','Charu','Deepak'],\n\"Maths\":[34,43,56,77],\n\"History\":[23,45,67,82],\n\"Hindi\":[53,35,np.nan,\"bye\"]},\nindex = range(21,25))",
        "question": "What is the output of the following code snippet:\nprint(df.Hindi.apply(type).nunique())",
        "options": null,
        "correctOption": 3,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following common data and answer the subquestion:\ndf=pd.DataFrame(data={'Name':['Akash','Brajesh','Charu','Deepak'],\n\"Maths\":[34,43,56,77],\n\"History\":[23,45,67,82],\n\"Hindi\":[53,35,np.nan,\"bye\"]},\nindex = range(21,25))",
        "question": "What is the output of the following code snippet:\ndef getPassing(aRow):\n    if aRow['Maths']>=40 and aRow['History']>=40:\n        return True\n    return False\ndf.apply(getPassing).sum()",
        "options": null,
        "correctOption": -1,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following common data and answer the subquestion:\ndf=pd.DataFrame(data={'Name':['Akash','Brajesh','Charu','Deepak'],\n\"Maths\":[34,43,56,77],\n\"History\":[23,45,67,82],\n\"Hindi\":[53,35,np.nan,\"bye\"]},\nindex = range(21,25))",
        "question": "What is the output of the following code snippet:\ndf=df.dropna()",
        "options": {
            "A": "The number of rows will decrease in the dataset.",
            "B": "The number of columns will decrease in the dataset.",
            "C": "The number of columns and number of rows, both, will decrease in the dataset.",
            "D": "There will be no change.",
            "E": "Insufficient information."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "To load datasets from openml.org, which method will be appropriate?",
        "options": {
            "A": "load_openml()",
            "B": "read_openml()",
            "C": "read_data()",
            "D": "fetch_openml()",
            "E": "load_data()",
            "F": "load_csv()",
            "G": "fetch_csv()"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "In which of the option given below data preprocessing is required?",
        "options": {
            "A": "In some columns which has values between 0 and 1.",
            "B": "A column contains entity names such as 'Chennai', 'CHENNAI' and 'MADRAS'.",
            "C": "The data has only numbers in all the columns."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code block:\nX = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits = 3)\nfor train, test in kf.split(X):\n    print(train, test)",
        "question": "Which of the following may be the correct output of the above code?",
        "options": {
            "A": "[3 4 5 6 7 8] [0 3 4]\n[0 1 2 6 7 8] [2 4 6]\n[0 1 2 3 4 5] [0 5 8]",
            "B": "[3 4 5 6 7 8] [0 1 2]\n[0 1 2 6 7 8] [3 4 5]\n[0 1 2 3 4 5] [6 7 8]",
            "C": "[3 4 5] [6 7 8] [0 1 2]\n[0 1 2] [6 7 8] [3 4 5]\n[0 1 2] [3 4 5] [6 7 8]",
            "D": "[0 1 2] [3 4 5 6 7 8]\n[3 4 5] [0 1 2 6 7 8]\n[6 7 8] [0 1 2 3 4 5]"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Which of the following scikit-learn classes is best suited for examining how the number of samples impacts training and testing errors?",
        "options": {
            "A": "sklearn.model_selection.train_test_split",
            "B": "sklearn.model_selection.cross_validate",
            "C": "sklearn.model_selection.cross_val_score",
            "D": "sklearn.model_selection.learning_curve"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Consider following two statements:\nStatement 1: The GaussianNB classifier can incrementally learn using partial_fit.\nStatement 2: GaussianNB performance suffers when features are dependent, as it assumes independence in calculating conditional probabilities.",
        "options": {
            "A": "Both statements are True",
            "B": "Only statement 1 is True",
            "C": "Only statement 2 is True",
            "D": "Both statements are False"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Given below code to load a huge file name as filename.csv and this file is not loading at once in the system which parameter should be added to pd.read_csv to load this file ?\nimport pandas as pd\nfrom sklearn.linear_model import SGDRegressor\nfor train_df in pd.read_csv(\"filename.csv\", __________=1024):\n    X = train_df.iloc[:, :-1]\n    y = train_df.iloc[:, -1]\n    model = SGDRegressor()\n    model.partial_fit(prep_X,y)",
        "question": "Given below code to load a huge file name as filename.csv and this file is not loading at once in the system which parameter should be added to pd.read_csv to load this file ?",
        "options": {
            "A": "max_depth",
            "B": "C",
            "C": "chunksize",
            "D": "warm_start"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following is true for a hard margin SVM algorithm ?",
        "options": {
            "A": "It does not create hyperplanes as a decision boundary",
            "B": "It will correctly classify all the data point if the data is linearly separable",
            "C": "It is robust to outliers",
            "D": "It is mostly used for clustering the data"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "What could be the output for below code\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = ['This is the first document.',\n          'This document is the second document.']\nvectorizer = CountVectorizer()\nvectorizer.fit_transform(corpus)\nprint(vectorizer.vocabulary_)",
        "options": {
            "A": "['document' 'first' 'is' 'second' 'the' 'this']",
            "B": "{'this': 5, 'is': 2, 'the': 4, 'first': 1, 'document': 0, 'second': 3}",
            "C": "[5, 2, 4, 1, 0, 3]",
            "D": "[[1, 1, 1, 0, 0, 1, 1],\n [2, 0, 1, 1, 1, 1]]"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Decision Trees are prone to:",
        "options": {
            "A": "Low bias, low variance",
            "B": "High bias, low variance",
            "C": "Low bias, high variance",
            "D": "High bias, high variance"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Following is the code to tune the degree parameter of a polynomial regression model.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import SGDRegressor\n\nparam_grid = [{'__________': [2, 3, 4, 5, 6, 7, 8, 9]}]\npipeline = Pipeline(steps=[('poly', PolynomialFeatures()),\n                           ('sgd', SGDRegressor())])\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X_train, y_train)",
        "question": "What should the blank space contain?",
        "options": {
            "A": "'degree'",
            "B": "'sgd_degree'",
            "C": "'sgd__degree'",
            "D": "'poly_degree'",
            "E": "'poly__degree'"
        },
        "correctOption": "E",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code block:\nfrom sklearn.datasets import make_regression\nX, y = make_regression(n_samples = 1000,\n                       n_features = 4,\n                       n_informative = 2,\n                       random_state=42)\n\nfrom sklearn.linear_model import SGDRegressor\nsgd1 = SGDRegressor(alpha=1e-3,\n                    random_state=42\n                    penalty='____________',)\nsgd1.fit(X, y)\nprint(sgd1.coef_)\n\nsgd2 = SGDRegressor(alpha=1e-3,\n                    random_state=42,\n                    penalty='____________')\nsgd2.fit(X, y)\nprint(sgd2.coef_)\n\nWhat are the most suitable values to be filled in the two blank spaces (in that order) in the code to expect the following output?:\n[48.50064306, 0, 0, 8.53931469]\n[4.84494867e+01, -7.58443057e-04, -2.09844306e-03, 8.53220113e+00]",
        "question": "What are the most suitable values to be filled in the two blank spaces (in that order) in the code to expect the following output?:\n[48.50064306, 0, 0, 8.53931469]\n[4.84494867e+01, -7.58443057e-04, -2.09844306e-03, 8.53220113e+00]",
        "options": {
            "A": "'l1', 'l2'",
            "B": "'l1', None",
            "C": "'l2', 'l1'",
            "D": "'l2', None"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider following code:\nestimator = SGDClassifier(loss='log_loss',\n                          penalty='l2',\n                          max_iter=1,\n                          warm_start='_____',\n                          eta0=0.01,\n                          alpha=0,\n                          learning_rate='constant',\n                          )\npipe_sgd = make_pipeline(MinMaxScaler(), estimator)",
        "question": "Which of the following is a suitable choice for warm_start if you want to plot learning curve with decreasing loss when trained for 100 epochs? Make necessary assumptions.",
        "options": {
            "A": "True",
            "B": "False",
            "C": "Yes",
            "D": "No",
            "E": "None of these"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "How does the learning_rate parameter affect the AdaBoost algorithm?",
        "options": {
            "A": "It controls the number of weak learners.",
            "B": "It shrinks the contribution of each classifier.",
            "C": "It sets the weight of each weak learner.",
            "D": "It adjusts the speed at which the model learns."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "The following code produces an output of 0.9125. How is the output expected to change if we increase the max_depth value?:\nfrom sklearn.datasets import load_wine\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX,y = load_wine(as_frame = True, return_X_y = True)\nX_train,X_test,y_train,y_test = train_test_split(X,\n                                                  y,\n                                                  test_size = 0.10,\n                                                  random_state = 12)\n\nclf = DecisionTreeClassifier(max_depth = 2,\n                             min_samples_split = 2,\n                             min_samples_leaf=3,\n                             random_state = 81)\n\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))",
        "question": "The following code produces an output of 0.9125. How is the output expected to change if we increase the max_depth value?",
        "options": {
            "A": "Output score is likely to increase.",
            "B": "Output score is likely to decrease.",
            "C": "Output score may increase or decrease.",
            "D": "Output score will remain the same."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Consider the following code. How many different combinations of Decision-TreeClassifier models will be trained internally?\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [{'max_depth':range(1, 10, 2),\n               'min_samples_split': range(1, 10, 3)}]\ngs = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 5)\ngs.fit(X,y)",
        "options": {
            "A": "20",
            "B": "75",
            "C": "8",
            "D": "15",
            "E": "40"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Consider the following code for a VotingClassifier. What is the effect of setting voting='soft'??\nfrom sklearn.ensemble import VotingClassifier\nclf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = SVC(probability=True)\neclf = VotingClassifier(estimators=[('lr', clf1),\n                                   ('rf', clf2),\n                                   ('svc', clf3)],\n                        voting='soft')\neclf.fit(X_train, y_train)",
        "options": {
            "A": "The final predictions are based on the majority vote.",
            "B": "The final predictions are based on the average of probabilities predicted by each classifier.",
            "C": "The final predictions are based on the weighted sum of the predictions.",
            "D": "The final predictions are based on the classifier with the highest accuracy."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Given the following code, what will be the output of print(scores.mean())?\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nclf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=50)\nscores = cross_val_score(clf, X, y, cv=5)\nprint(scores.mean())",
        "options": {
            "A": "The mean accuracy across all cross-validation folds.",
            "B": "The mean precision across all cross-validation folds.",
            "C": "The mean recall across all cross-validation folds.",
            "D": "The mean F1 score across all cross-validation folds."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "Which of the following approach(es) is(are) helpful to find a good value for n_clusters in KMeans clustering algorithm?",
        "options": {
            "A": "Plotting the sum of the squared distances of samples to their closest cluster center",
            "B": "By Changing the distance metric for KMeans",
            "C": "By initialising clustering centroids very far from each other",
            "D": "All of these"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code:\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nX, y = load_iris(return_X_y = True)\n\nThe sizes of X and y are (150, 4) and (150,) respectively.",
        "question": "Which of the following would be the correct code snippet to split X and y into training and test data such that test data has exactly 30 samples?",
        "options": {
            "A": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=30)",
            "B": "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=\"20%\")",
            "C": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)",
            "D": "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, )"
        },
        "correctOption": [
            "A",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code block:\nfrom sklearn.linear_model import linear_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\nlin_reg = linear_regression()\nshuffle_split = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nscore = cross_val_score(lin_reg, X, y, cv=shuffle_split, scoring='____________')",
        "question": "Which of the following may be appropriate to be filled in the blank space?",
        "options": {
            "A": "r2",
            "B": "neg_r2",
            "C": "mean_absolute_error",
            "D": "neg_mean_absolute_error"
        },
        "correctOption": [
            "A",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Which of the following is a hyper parameter?",
        "options": {
            "A": "'intercept_' in LinearRegression",
            "B": "'degree' in PolynomialFeatures",
            "C": "'n_neighbors' in KNeighborsClassifier",
            "D": "'max_depth' of tree in a DecisionTreeClassifier"
        },
        "correctOption": [
            "B",
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following models are inherently multiclass models?",
        "options": {
            "A": "Perceptron",
            "B": "DecisionTreeClassifier",
            "C": "KNeighborClassifier",
            "D": "LogisticRegression"
        },
        "correctOption": [
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following class(es) is/are used to instantiate a neural network in Sklearn.",
        "options": {
            "A": "SGDClassifier()",
            "B": "MLPClassifier()",
            "C": "NNClassifier()",
            "D": "MLPRegressor()"
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following is correct?",
        "options": {
            "A": "SGDClassifier(loss=\"perceptron\") is stochastic version of a perceptron model.",
            "B": "SGDClassifier(loss=\"hinge\") is stochastic version of a DecisionTree classifier model.",
            "C": "SGDClassifier(loss=\"multinomial\") is stochastic version of a softmax classifier model.",
            "D": "SGDClassifier(loss=\"log_loss\") is stochastic version of a logistic classifier model."
        },
        "correctOption": [
            "A",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Which of the following processes can be done if we have a dataset with imbalanced target class distribution?",
        "options": {
            "A": "Remove all the minority classes",
            "B": "Up-sample the minority classes",
            "C": "Remove all the majority classes",
            "D": "Down-sample the majority classes",
            "E": "create synthetic samples to balance the classes"
        },
        "correctOption": [
            "B",
            "D",
            "E"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following is true about Naive Bayes algorithm ?",
        "options": {
            "A": "It is primarily used for regression problems",
            "B": "It is primarily used for classification problems",
            "C": "Hyperparameter tuning is required",
            "D": "Hyperparameter tuning is not required"
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following is/are correct regarding RadiusNeighborsClassifier",
        "options": {
            "A": "Only 5 neighbours in the range of some radius are used to compute the label of a sample.",
            "B": "All the neighbours in the range of some radius are used to compute the label of a sample.",
            "C": "It is sensitive to outliers.",
            "D": "It is not sensitive to outliers."
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "In which of the following cases we should use partial_fit instead of fit method ?",
        "options": {
            "A": "Data is continuously being generated",
            "B": "Data is generated every month",
            "C": "Whole data is generated and its in a huge file size",
            "D": "For very small dataset"
        },
        "correctOption": [
            "A",
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "The following line of code create a neural network (make necessary assumptions)\nfrom sklearn.neural_network import MLPRegressor\nregr = MLPRegressor(hidden_layer_sizes=(5,3),max_iter=5)\nregt.fit(X_train, Y_train)",
        "question": "Select the correct statements from the following list of statements",
        "options": {
            "A": "The neural network contains 3 hidden layers with 5 neurons in each hidden layer",
            "B": "The neural network contains 5 hidden layers with 3 neurons in each hidden layer",
            "C": "The neural network contains 2 hidden layers with 3 neurons in the second hidden layer",
            "D": "The neural network contains 2 hidden layers with 5 neurons in the first hidden layer",
            "E": "All of the given options are correct"
        },
        "correctOption": [
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Consider the following block of code:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX,y = load_breast_cancer(as_frame = True, return_X_y = True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,\n                                                  test_size = 0.2,\n                                                  random_state=42)\n\nclf = DecisionTreeClassifier(min_samples_split = 6,\n                             min_samples_leaf = 4,\n                             random_state = 5)\n\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))",
        "question": "In which of the following scenarios, the split will can happen at node N?",
        "options": {
            "A": "Number of samples at node N = 15. If it is split, it will result in 9 samples in the left child node and 6 sample in the right child node.",
            "B": "Number of samples at node N = 5. If it is split, it will result in 4 samples in the left child node and 2 samples in the right child node.",
            "C": "Number of samples at node N = 12. If it is split, it will result in 3 samples in the left child node and 9 samples in the right child node.",
            "D": "Number of samples at node N = 7. If it is split, it will result in 4 samples in the left child node and 3 samples in the right child node."
        },
        "correctOption": [
            "A",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "What will be the output of the following code?: (upto 3 decimals)\nfrom sklearn.metrics import mean_absolute_error\ny_true = [0.5, 0.2, 0.7, 1]\ny_pred = [0.4, 0.2, 1, 0.1]\nmean_absolute_error(y_true, y_pred)",
        "options": null,
        "correctOption": {
            "min": 0.319,
            "max": 0.328
        },
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "The plot below displays a distribution of 180 samples with 2 features in Euclidean space. If we apply the KMeans clustering algorithm to these data points, what is the value of n_clusters for which the inertia (sum of squared errors) will be exactly zero? If you conclude,",
        "options": null,
        "correctOption": 180,
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764053886/may2024-1_inilxl.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "For a dataset with 1000 data points and 100 features, the following code will generate how many models during execution?\nNote: X is the feature matrix and y is the target vector.\n\npython\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nloocv = LeaveOneOut()\nscore = cross_val_score(lin_reg, X, y, cv=loocv)\n",
        "question": "For a dataset with 1000 data points and 100 features, the following code will generate how many models during execution?",
        "options": {
            "A": "1000",
            "B": "100",
            "C": "99",
            "D": "999"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Neha created the following classifier:\nShe is working with a large, dense dataset where both n_samples and n_features are large. According to scikit-learn's solver selection rules for solver='auto', which solver will most likely be chosen in this case?",
        "options": {
            "A": "'cholesky'",
            "B": "'sag'",
            "C": "'lsqr'",
            "D": "'sparse_cg'"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "You want to implement an MLPClassifier with hidden_layer_sizes=(100,), activation='tanh', solver='sgd', alpha=0.001, learning_rate_init=0.01, max_iter=500, random_state=0. Which code snippet is correct?",
        "options": {
            "A": "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='tanh', solver='sgd', alpha=1e-3, learning_rate_init=0.01, max_iter=500, random_state=0)\nmlp.fit(X_train, y_train)",
            "B": "mlp = MLPClassifier(activation='tanh', solver='sgd', alpha=1e-3, learning_rate_init=0.01, max_iter=500, random_state=0)\nmlp.fit(X_train, y_train)",
            "C": "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='sgd', alpha=1e-3, learning_rate_init=0.01, max_iter=500, random_state=0)\nmlp.fit(X_train, y_train)",
            "D": "mlp = MLPRegressor(hidden_layer_sizes=(100,), activation='tanh', solver='sgd', alpha=1e-3, learning_rate_init=0.01, max_iter=500, random_state=0)\nmlp.fit(X_train, y_train)"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "What is the primary function of the pandas.resample() method for a time series?",
        "options": {
            "A": "To convert a DataFrame to a series or vice versa.",
            "B": "To fill in missing values in the time series using various interpolation methods.",
            "C": "To change the frequency of the time series, such as converting daily data to monthly or monthly data to daily.",
            "D": "To perform statistical tests to check for stationarity in the time series."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following confusion matrix for a 3-class classification problem.\n\n| True\\Pred | Class 1 | Class 2 | Class 3 |\n|---|---|---|---|\n| Class 1 | 11 | 2 | 1 |\n| Class 2 | 4 | 9 | 3 |\n| Class 3 | 0 | 4 | 11 |",
        "question": "Which of the classes has the highest value of Recall?",
        "options": {
            "A": "Class 1",
            "B": "Class 2",
            "C": "Class 3",
            "D": "Recall cannot be calculated for a specific class"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Given below are a few commonly used operations while handling image data. Match the blanks with the options provided.\n\nThe images can be converted from RGB to ____(i)____ to reduce the number of channels.\nSince input images can have different dimensions, we ____(ii)____ them to ensure a consistent shape across the dataset.\nAfter standardizing their size, we ____(iii)____ each image to convert it into a one-dimensional array suitable for model input.\nTo improve the model's ability to generalize, we can ____(iv)____ the dataset by generating variations of the original images using transformations like rotation, zooming, and cropping.",
        "options": {
            "A": "(i) - resize, (ii) - augment, (iii) - flatten, (iv) - grayscale",
            "B": "(i) - grayscale, (ii) - flatten, (iii) - resize, (iv) - augment",
            "C": "(i) - grayscale, (ii) - augment, (iii) - resize, (iv) - flatten",
            "D": "(i) - grayscale, (ii) - resize, (iii) - flatten, (iv) - augment"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "A time series with a clear upward trend and increasing variance over time is identified as non-stationary. Which two preprocessing steps are most likely required?",
        "options": {
            "A": "Min-Max scaling and seasonal decomposition",
            "B": "Imputing missing values and differencing",
            "C": "Log transformation and differencing",
            "D": "Standardization and moving average smoothing"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "In SGDClassifier, setting loss='log' will make it equivalent to:",
        "options": {
            "A": "LogisticRegression(solver='sgd')",
            "B": "Perceptron()",
            "C": "LinearSVC()",
            "D": "RidgeClassifier()",
            "E": "None of these"
        },
        "correctOption": "E",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "You want to implement KMeans with number of clusters as 3, initialization done using 'k-means++' and number of initializations as 10 with random state of 42. Which code snippet is correct?",
        "options": {
            "A": "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)\nkmeans.fit(X)",
            "B": "kmeans = KMeans(init='k-means++', n_init=10, random_state=42)\nkmeans.fit(X)",
            "C": "kmeans = KMeans(n_clusters=3, init='random', n_init=10, random_state=42)\nkmeans.fit(X)",
            "D": "kmeans = KMeans(n_clusters=3, init='k-means', n_init=10, random_state=42).fit(X)"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "You want to build an sklearn pipeline that first imputes missing values using SimpleImputer (mean strategy) and then standardizes features with StandardScaler. Which of the following code snippets correctly create such a pipeline?",
        "options": {
            "A": "from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\npipe = Pipeline(steps=[\n \u00a0 \u00a0(\"imputer\", SimpleImputer(strategy=\"mean\")),\n \u00a0 \u00a0(\"scaler\", StandardScaler())\n])",
            "B": "from sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\npipe = make_pipeline(SimpleImputer(strategy=\"mean\"), StandardScaler())",
            "C": "from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\npipe = Pipeline([\n \u00a0 \u00a0(\"scaler\", StandardScaler()),\n \u00a0 \u00a0(\"imputer\", SimpleImputer(strategy=\"median\"))\n])",
            "D": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\npipe = Pipeline([(\"scaler\", StandardScaler())]).fit_transform(X)",
            "E": "None of these"
        },
        "correctOption": [
            "A",
            "B"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "In scikit-learn, many estimators and transformers provide the option to enable caching (e.g., using the memory parameter in Pipeline). Which of the following are advantages of enabling caching?",
        "options": {
            "A": "Avoids recomputing intermediate results when the same transformation is applied multiple times during fitting.",
            "B": "Reduces the overall computation time for repeated pipeline executions with the same parameters and data.",
            "C": "Automatically reduces the memory footprint by deleting intermediate results after every step.",
            "D": "Allows faster hyperparameter tuning with GridSearchCV or RandomizedSearchCV when transformations are reused.",
            "E": "Guarantees higher model accuracy by reusing previous computations."
        },
        "correctOption": [
            "A",
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Examine the following Python code snippet:\n\npython\nfrom sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor(learning_rate='adaptive', eta0=0.01)\nsgd.fit(X_train, y_train)\n",
        "question": "Which of the following statements correctly describe the behavior of the above code?",
        "options": {
            "A": "The learning rate remains constant while the training loss continues to decrease.",
            "B": "If the loss does not improve, the learning rate is decreased by a factor of 5.",
            "C": "Training terminates as soon as the learning rate becomes smaller than 1 x 10\u207b\u2076.",
            "D": "This configuration always converges faster than the 'invscaling' learning rate option."
        },
        "correctOption": [
            "A",
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "A binary classifier is trained on a dataset where classes are highly imbalanced. To handle this imbalance during training, the parameter class_weight is set to 'balanced' in scikit-learn. Which of the following statements about this setting are correct?",
        "options": {
            "A": "It assigns weights to classes inversely proportional to their frequencies.",
            "B": "It penalizes mistakes in the minority class more heavily.",
            "C": "It forces both classes to have exactly the same number of samples.",
            "D": "It can be applied to LogisticRegression, SGDClassifier, and RidgeClassifier."
        },
        "correctOption": [
            "A",
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "In the context of KMeans clustering, which of the following statements about the Elbow Method are correct?",
        "options": {
            "A": "It helps determine the optimal number of clusters by plotting the number of clusters vs. inertia (within-cluster sum of squares).",
            "B": "The 'elbow point' is where adding more clusters yields a significantly larger reduction in inertia.",
            "C": "The 'elbow point' is where adding more clusters yields diminishing returns in reducing inertia.",
            "D": "It always produces a clear and unambiguous elbow point.",
            "E": "It can be visualized using a plot of number of clusters vs. distortion score."
        },
        "correctOption": [
            "A",
            "C",
            "E"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "How to disable early stopping in SGDRegressor?",
        "options": {
            "A": "SGDRegressor(early_stopping=False)",
            "B": "SGDRegressor(stop_early=False)",
            "C": "SGDRegressor(tol=None)",
            "D": "SGDRegressor(disable_stop=True)"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Given below are the true values of the target y_true and predicted values of the target y_pred.\n\npython\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\n",
        "question": "Calculate and enter the Mean Absolute Error (Rounded to two decimal places).",
        "options": null,
        "correctOption": 0.5,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following code snippet for transforming text data using CountVectorizer\n\npython\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n \u00a0 \u00a0'This is the first document',\n \u00a0 \u00a0'This document is the second document'\n]\n\nvectorizer = CountVectorizer(lowercase=True)\nX = vectorizer.fit_transform(corpus)\n\nX_array = X.toarray()\n",
        "question": "What is the sum of all the values in X_array?",
        "options": null,
        "correctOption": 11,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following code snippet, assume necessary imports:\n\npython\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n \u00a0 \u00a0\"Name\": [\"Amit\", \"Bhavna\", \"Chetan\", \"Divya\", \"Ekta\"],\n \u00a0 \u00a0\"Maths\": [78, None, 92, 85, 60],\n \u00a0 \u00a0\"Science\": [88, 90, None, None, 75],\n \u00a0 \u00a0\"Section\": [\"A\", \"A\", \"B\", \"B\", \"A\"]\n})\n\n\nBased on the above data, answer the given subquestions.",
        "question": "Which code snippet will correctly select the Name and Math columns for students in Section \"B\"?",
        "options": {
            "A": "df[df[\"Section\"]==\"B\"][[\"Name\", \"Maths\"]]",
            "B": "df.loc[df[\"Section\"]==\"B\",[\"Name\", \"Maths\"]]",
            "C": "df.query(\"Section == 'B'\")[['Name', 'Maths']]",
            "D": "df[df[\"Section\"]==\"B\"][(\"Name\", \"Maths\")]"
        },
        "correctOption": [
            "A",
            "B"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following code snippet, assume necessary imports:\n\npython\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n \u00a0 \u00a0\"Name\": [\"Amit\", \"Bhavna\", \"Chetan\", \"Divya\", \"Ekta\"],\n \u00a0 \u00a0\"Maths\": [78, None, 92, 85, 60],\n \u00a0 \u00a0\"Science\": [88, 90, None, None, 75],\n \u00a0 \u00a0\"Section\": [\"A\", \"A\", \"B\", \"B\", \"A\"]\n})\n\n\nBased on the above data, answer the given subquestions.",
        "question": "Which of the following will drop all rows where Maths is missing?",
        "options": {
            "A": "df.dropna(subset=[\"Maths\"])",
            "B": "df.dropna(how=\"any\")",
            "C": "df[df[\"Maths\"].isnull()]",
            "D": "df.dropna(axis=1, subset=[\"Maths\"])"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following code snippet, assume necessary imports:\n\npython\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n \u00a0 \u00a0\"Name\": [\"Amit\", \"Bhavna\", \"Chetan\", \"Divya\", \"Ekta\"],\n \u00a0 \u00a0\"Maths\": [78, None, 92, 85, 60],\n \u00a0 \u00a0\"Science\": [88, 90, None, None, 75],\n \u00a0 \u00a0\"Section\": [\"A\", \"A\", \"B\", \"B\", \"A\"]\n})\n\n\nBased on the above data, answer the given subquestions.",
        "question": "You want to create a new column Total that stores the sum of Maths and Science, treating missing values as zeros. Which of the following will work?",
        "options": {
            "A": "df[\"Total\"] = df[[\"Maths\", \"Science\"]].fillna(0).sum(axis=1)",
            "B": "df[\"Total\"] = df.Maths.add(df.Science, fill_value=0)",
            "C": "df[\"Total\"] = df[\"Maths\"] + df[\"Science\"]",
            "D": "df.assign(Total=lambda x: x[\"Maths\"].add(x[\"Science\"], fill_value=0))"
        },
        "correctOption": [
            "A",
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Consider the code snippet below for performing hyperparameter tuning on a RandomForestRegressor using GridSearchCV\n\npython\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n \u00a0 \u00a0'n_estimators': [20, 50, 100],\n \u00a0 \u00a0'criterion': [A, B],\n \u00a0 \u00a0'max_depth': [5, 10, 20]\n}\n\ngrid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring=C)\n\n\nBased on the above data, answer the given subquestions.",
        "question": "select the appropriate values for A and B in the criterion parameter to ensure that all model fits execute successfully.",
        "options": {
            "A": "\"gini\", \"entropy\"",
            "B": "\"gini\", \"log_loss\"",
            "C": "\"squared_error\", \"entropy\"",
            "D": "\"squared_error\", \"absolute_error\""
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the code snippet below for performing hyperparameter tuning on a RandomForestRegressor using GridSearchCV\n\npython\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n \u00a0 \u00a0'n_estimators': [20, 50, 100],\n \u00a0 \u00a0'criterion': [A, B],\n \u00a0 \u00a0'max_depth': [5, 10, 20]\n}\n\ngrid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring=C)\n\n\nBased on the above data, answer the given subquestions.",
        "question": "Which of the following scoring metrics can be provided in the place of C while initializing Grid Search?",
        "options": {
            "A": "\"entropy\"",
            "B": "\"r2\"",
            "C": "\"weighted_f1\"",
            "D": "\"neg_mean_absolute_error\""
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the code snippet below for performing hyperparameter tuning on a RandomForestRegressor using GridSearchCV\n\npython\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n \u00a0 \u00a0'n_estimators': [20, 50, 100],\n \u00a0 \u00a0'criterion': [A, B],\n \u00a0 \u00a0'max_depth': [5, 10, 20]\n}\n\ngrid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring=C)\n\n\nBased on the above data, answer the given subquestions.",
        "question": "After making the appropriate substitutions for A, B and C, how many RandomForestRegressor models will be fit while searching for the best parameters?",
        "options": null,
        "correctOption": 90,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "Based on the below data, answer the given subquestions.",
        "question": "Calculate and enter the value of X (truncated to two decimal places).",
        "options": null,
        "correctOption": {
            "min": 0.97,
            "max": 0.99
        },
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764054093/may2024-2_cksjh9.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "Based on the below data, answer the given subquestions.",
        "question": "Calculate and enter the value of Y (truncated to two decimal places).",
        "options": null,
        "correctOption": {
            "min": 0.77,
            "max": 0.79
        },
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764054093/may2024-2_cksjh9.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "May 2025",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "Based on the below data, answer the given subquestions.",
        "question": "Using the User-User similarity matrix, apply the User-User collaborative filtering method to predict the missing rating of Title C for Robin. Provide your answer truncated to three decimal places.",
        "options": null,
        "correctOption": {
            "min": 3.8,
            "max": 4.1
        },
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764054093/may2024-2_cksjh9.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the following Python code snippet using Pandas:\n\npython\nimport pandas as pd\n\n# Assume a DataFrame df is already loaded with appropriate data.\n\ngrouped_data = df.groupby('Category')['Quantity'].sum()\n",
        "question": "What does the groupby operation in the code achieve?",
        "options": {
            "A": "It calculates the total quantity for each category.",
            "B": "It calculates the average quantity for each category.",
            "C": "It groups the data by the 'Category' column.",
            "D": "It filters out rows where the 'Quantity' column is zero."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider the below code:\n\npython\ndata = [[-6, 5],\n \u00a0 \u00a0 \u00a0 \u00a0[-6, 5],\n \u00a0 \u00a0 \u00a0 \u00a0[ 3, 1],\n \u00a0 \u00a0 \u00a0 \u00a0[ 3, 1]]\n\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nprint(ss.fit_transform(data))\n",
        "question": "Which of the following option represents the print output :",
        "options": {
            "A": "[[0, -1],\n [0, -1],\n [1, 1],\n [1, 1]]",
            "B": "[[-0.5, -2],\n [-0.5, -2],\n [ 1,  2],\n [ 1,  2]]",
            "C": "[[ -1, 1],\n [ -1, 1],\n [ 1, -1],\n [ 1, -1]]",
            "D": "[[-1, -1],\n [-1, -1],\n [ 1, 1],\n [ 1, 1]]"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the Ridge regression model in scikit-learn, represented by the Ridge class.",
        "question": "Which of the following statements about Ridge regression is correct?",
        "options": {
            "A": "Ridge regression is specifically designed for handling non-linear relationships in the data.",
            "B": "The regularization term in Ridge regression is added to the sum of squared residuals.",
            "C": "Increasing the value of the regularization parameter (alpha) in Ridge regression tends to overfit the model.",
            "D": "Ridge regression is equivalent to ElasticNet regression when the regularization parameter (alpha) is set to one."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Why is it relevant to add a preprocessing step to scale the data using a StandardScaler when working with a KNeighborsClassifier?",
        "options": {
            "A": "Speeds up the process of finding neighbors on unscaled data.",
            "B": "K-nearest neighbors relies on computing distances. Normalizing features ensures that each feature contributes approximately equally to the distance computation.",
            "C": "Scaling the data significantly improves the accuracy of k-nearest neighbor models.",
            "D": "It doesn't matter. K-nearest neighbors works equally well with or without normalizing the dataset."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "When using CountVectorizer in scikit-learn, what does the max_features parameter control?",
        "options": {
            "A": "The upper limit on the number of documents considered during vectorization.",
            "B": "The maximum number of features (words) to be extracted based on term frequency.",
            "C": "The maximum number of randomly selected features (words).",
            "D": "The upper limit on the number of characters allowed in each document."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "Rahul is working on an unsupervised machine learning algorithm. He is not able to choose the optimum number of clusters(k) for his model (model is based on K- means algorithm). He tried to plot the elbow chart which is shown below.",
        "question": "By looking at this chart, which option would you recommend Rahul as the most suitable value of number of clusters (k).",
        "options": {
            "A": "4",
            "B": "8",
            "C": "12",
            "D": "40",
            "E": "44"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764054310/sept2023-1_ovspdz.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "You're training a multi-layer perceptron (MLP) classifier on a dataset for a multi-class classification task. The following code snippet demonstrates the process using the 'MLPClassifier' from scikit-learn:\n\npython\nfrom sklearn.neural_network import MLPClassifier\nimport numpy as np\n\n# Simulated data (features: X, target: y)\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 2])\n\n# Create an MLPClassifier with a specified maximum number of iterations\nmodel = MLPClassifier(max_iter=100, alpha = 0.01, random_state=42)\n\n# Fit the model on the training data\nmodel.fit(X, y)\n\n# Get the number of iterations used in training\niterations_used = model.n_iter_\n",
        "question": "What does the parameter 'alpha' in the MLPClassifier signify?",
        "options": {
            "A": "It determines the maximum fraction of neurons to be used to train the model",
            "B": "It is the learning rate",
            "C": "It is the L1 regularization rate.",
            "D": "It is the L2 regularization rate.",
            "E": "None of these"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Suppose you have loaded a dataset representing various attributes of red wine into a Pandas DataFrame named wine_data using the following code:\n\npython\nimport pandas as pd\n\ndata_url = '''https://archive.ics.uci.edu/ml/machine-learning-databases/\nwine-quality/winequality-red.csv'''\nwine_data = pd.read_csv(data_url, sep=';')\n\n\nNow, suppose you want to retrieve the value of the alcohol attribute for the fifth sample in the dataset (4th by index). The alcohol attribute is at the 3rd index of columns.",
        "question": "Which of the following expressions correctly achieves this?",
        "options": {
            "A": "wine_data.alcohol[4]",
            "B": "wine_data['alcohol'][4]",
            "C": "wine_data[4][3]",
            "D": "wine_data.iloc[4, 3]"
        },
        "correctOption": [
            "A",
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You're building a machine learning pipeline to preprocess data and train a model on a classification task. You decide to use a pipeline that includes data pre-processing and a support vector machine (SVM) classifier. The following code snippet demonstrates the pipeline creation and usage:\n\npython\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Simulated data (features: X, target: y)\nX = np.array([[2, 3], [5, 7], [8, 10]])\ny = np.array([0, 1, 0])\n\n# Create a pipeline with StandardScaler and SVM classifier\npipe = Pipeline([\n \u00a0 \u00a0('scaler', StandardScaler()),\n \u00a0 \u00a0('svm', SVC())\n])\n\n# Fit the pipeline on training data\npipe.fit(X, y)\n\n# Make predictions using the trained pipeline\npredictions = pipe.predict(X)\n",
        "question": "Which of the following can be used to get number of support vectors of the classifier?",
        "options": {
            "A": "pipe[1].n_support_",
            "B": "pipe[1].n_support_vecs",
            "C": "pipe['svm'].n_support_",
            "D": "pipe['svm'].n_support_vecs",
            "E": "None of these"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You are building a sentiment analysis model using scikit-learn's SGDClassifier to classify movie reviews as positive or negative. The dataset is quite large, and you're dealing with a high-dimensional feature space. You want to fine-tune the hyperparameters of the classifier to achieve better convergence and classification performance.\nHere's how you're setting up the SGDClassifier:\n\npython\nfrom sklearn.linear_model import SGDClassifier\nclassifier = SGDClassifier(loss='hinge', alpha=0.0001, \n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_iter=1000, tol=1e-3, power_t=0.5)\n",
        "question": "Which of the following are correct?",
        "options": {
            "A": "The classifier is based on perceptron model.",
            "B": "The classifier is based on logistic regression model.",
            "C": "The learning rate decays every iteration.",
            "D": "The learning rate is 0.0001.",
            "E": "It will run exactly for 1000 iterations, which is independent of the data.",
            "F": "None of these."
        },
        "correctOption": [
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "When working with a KNeighborsClassifier in scikit-learn, consider the following scenarios.",
        "question": "Choose all the correct statements:",
        "options": {
            "A": "A higher value of n_neighbors tends to overfit the model.",
            "B": "Reducing the value of n_neighbors typically increases the risk of overfitting.",
            "C": "Opting for a small n_neighbors can make the model sensitive to noise in the data.",
            "D": "Increasing n_neighbors always enhances the model's ability to generalize to new, unseen data."
        },
        "correctOption": [
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "python\nfrom sklearn.svm import SVC\nclf = SVC(kernel = ______)\nclf.fit(X, y)\n",
        "question": "Fill in the missing parameter value in the following estimator that can be used to classify the data",
        "options": {
            "A": "'lasso'",
            "B": "'poly'",
            "C": "'scale'",
            "D": "'rbf'"
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following are true about multilayer perceptron model in sklearn?",
        "options": {
            "A": "It is an iterative algorithm/model.",
            "B": "It can be used to capture non-linear relationships between features and labels.",
            "C": "It can be used for regression as well as classification.",
            "D": "It can be used for clustering.",
            "E": "None of these"
        },
        "correctOption": [
            "A",
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "consider the below code : keep following symbols in mind:\n\n- >>> : Represents input code\n- # : Represents comment in a code\n- ... : Represents code continuation\n- Without any symbols at the beginning of a line then it is output of just above input line of code.\n\npython\n>>> from sklearn.feature_selection import SelectKBest, f_regression\n>>> from sklearn.datasets import load_diabetes\n>>> X,y = load_diabetes(return_X_y=True,as_frame=True)\n>>> print(X.shape)\n(442,10)\n\n>>> print(X.columns)\n['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n\n>>> skb = SelectKBest(f_regression, k=4)\n>>> X_selected = skb.fit_transform(X, y)\n\n>>> print(skb.scores_)\n[16.10, 0.81, 230.65, 106.52, 20.71, 13.74, 81.23, 100.06, 207.27, 75.39]\n\n>>> print(skb.pvalues_)\n[7.0e-05, 3.6e-01, 3.4e-42, 1.6e-22, 6.9e-06,\n2.3e-04, 6.1e-18, 2.3e-21, 8.8e-39, 7.5e-17]\n\n>>> print(skb.pvalues_.argsort())\n[2, 8, 3, 7, 6, 9, 4, 0, 5, 1]\n",
        "question": "Which of the following feature(s) will be selected in X_selected from X ?",
        "options": {
            "A": "age",
            "B": "bmi",
            "C": "bp",
            "D": "s3",
            "E": "s4",
            "F": "s5"
        },
        "correctOption": [
            "B",
            "C",
            "E",
            "F"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "You've built a linear regression model to predict the temperature of a particular place 5 days from today, based on today's humidity and wind speed. The model's coefficients for the features are as follows:\n\n- Coefficient for humidity: +2.4\n- Coefficient for wind speed: -2",
        "question": "What does the coefficient for \u201cwind speed\u201d (5000) represent in this context?",
        "options": {
            "A": "For each additional unit of increase in wind speed, the temperature is expected to decrease by 2 units.",
            "B": "For each additional unit of increase in wind speed, the temperature is expected to increase by 2 units.",
            "C": "Humidity has a stronger impact on salary than years of experience.",
            "D": "The coefficient doesn't have any meaningful interpretation in this scenario."
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code snippet using scikit-learn:\n\npython\nsgd_regressor = SGDRegressor()\n\nparam_dist = { 'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'alpha': loguniform(1e-5, 1e1),\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'penalty': ['l1', 'l2', 'elasticnet'],\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'epsilon': loguniform(1e-5, 1e-2),}\n\nrandom_search = RandomizedSearchCV(sgd_regressor,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 param_distributions=param_dist, n_iter=15, cv=3,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 scoring='neg_mean_squared_error')\nrandom_search.fit(X, y)\n\n\nAssume all the necessary imports and X, y to be the training dataset.",
        "question": "Which of the following statements about the given code are correct?",
        "options": {
            "A": "The n_iter parameter in RandomizedSearchCV controls the number of hyper-parameter combinations to try.",
            "B": "The actual number of combinations tried in the fit operation is 36.",
            "C": "The hyperparameter search space for the alpha parameter follows a log-uniform distribution.",
            "D": "The scoring metric used for the search is the negative mean squared error."
        },
        "correctOption": [
            "A",
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following estimators are exactly same?",
        "options": {
            "A": "SGDClassifier(loss=\"perceptron\",\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0eta0=1,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0learning_rate=\"constant\",\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0penalty=None)",
            "B": "Perceptron()",
            "C": "SGDClassifier(loss=\"percept\",\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0eta0=1,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0learning_rate=\"constant\",\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0penalty=None)",
            "D": "SGDRegressor(loss=\"percept\",\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 eta0=1,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 learning_rate=\"constant\",\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 penalty=None)",
            "E": "All of these are different."
        },
        "correctOption": [
            "A",
            "B"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "When employing a Support Vector Classifier (SVC) in scikit-learn with different values of the regularization parameter (C), how does the complexity of the decision boundary change?",
        "question": "Pick all the correct statements:",
        "options": {
            "A": "Smaller values of C result in a more complex decision boundary.",
            "B": "Larger values of C lead to a more complex decision boundary.",
            "C": "The decision boundary tends to become simpler with increasing values of C.",
            "D": "Excessively large values of C may result in overfitting."
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "Choose the correct statements.",
        "options": {
            "A": "Hierarchical agglomerative clustering does not need the initial number of clusters to group the data.",
            "B": "K-means clustering does not need the number of clusters to group the data.",
            "C": "K-means clustering can be agglomerative or divisive.",
            "D": "Hierarchical clustering can be agglomerative or divisive.",
            "E": "None of these"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "python\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\ndata =[[5, 8, 4],[8, 4, 1],[7, np.nan, 6],[4, 2, 3],[np.nan, 6, 6]]\nsi = SimpleImputer(missing_values= np.nan,strategy=\"mean\" )\nsi.fit(data)\nprint(si.statistics_)\n",
        "question": "What will be the output of the below code ?",
        "options": {
            "A": "[6,5,3.75]",
            "B": "[5,5,3]",
            "C": "[6,5,4]",
            "D": "[5,5,5]"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "You're working on a dataset with two features and a target variable. You decide to use polynomial regression with a degree of 3 to capture potential cubic relationships. The following code snippet demonstrates the process:\n\npython\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n\n# Simulated data (feature: X)\nX = np.array([1, 2, 3, 4])\n\n# Reshape the features\nX = X.reshape(-1, 2)\n\n# Transform features into polynomial features\npoly = PolynomialFeatures(degree=3)\nX_poly = poly.fit_transform(X)\n",
        "question": "What will be the shape of the X_poly matrix after transforming the feature 'X' into polynomial features of degree 3?",
        "options": {
            "A": "(2, 10)",
            "B": "(2, 11)",
            "C": "(2, 9)",
            "D": "(2, 8)"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code snippet using scikit-learn:\n\npython\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline([\n \u00a0 \u00a0('scaler', MinMaxScaler()),\n \u00a0 \u00a0('classifier', SVC())\n])\n\nparam_grid = {\n \u00a0 \u00a0'scaler__feature_range': [(0, 1), (1, 2)],\n \u00a0 \u00a0'classifier__C': [0.01, 0.1, 1],\n \u00a0 \u00a0'classifier__kernel': ['linear', 'poly'],\n \u00a0 \u00a0'classifier__degree': [2, 3]\n}\n\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n\nAssume that X_train and y_train are the training feature matrix and label vector, respectively.",
        "question": "Which of the following statements about the given code is correct?",
        "options": {
            "A": "The grid_search.score(X_train, y_train) will give the accuracy on the test dataset by the 4 folds in which the model gives the best parameters.",
            "B": "The scaler__feature_range hyperparameter is being tuned for the MinMaxScaler.",
            "C": "The pipeline always uses a polynomial kernel ('poly') as the kernel for the SVC classifier.",
            "D": "A total of 12 combinations of hyperparameters were tried during the Grid-SearchCV."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "python\nfrom sklearn import linear_model\nclf = linear_model.Lasso(alpha=0.1)\nclf.fit([[1,2], [2, 1], [1, 2, 3]])\n\nlinear_model.Lasso(alpha=0.1,max_iter=1000, tol=0.0001, warm_start=False,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fit_intercept=True)\nprint(clf.coef_)\n",
        "question": "Which of the following is likely to be the correct output of the code given below?",
        "options": {
            "A": "[0.85,0.1,0.05]",
            "B": "[1.05 0.35]",
            "C": "[3,2,1]",
            "D": "There are some mistakes in the 3rd /4th line of code, hence it will produce error."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Consider the following code:\n\npython\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_wine\nX,y = load_wine(as_frame = True, return_X_y = True)\n\ndtc1 = DecisionTreeClassifier(ccp_alpha = 0.0)\ndtc1.fit(X, y)\n\ndtc2 = DecisionTreeClassifier(ccp_alpha = 0.06)\ndtc2.fit(X, y)\n\ndtc3 = DecisionTreeClassifier(ccp_alpha = 0.1)\ndtc3.fit(X, y)\n\ndtc4 = DecisionTreeClassifier(ccp_alpha = 0.03)\ndtc4.fit(X, y)\n",
        "question": "Which model is likely to overfit the most?",
        "options": {
            "A": "dtc1",
            "B": "dtc2",
            "C": "dtc3",
            "D": "dtc4"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "Output of the following code snippet is mentioned below.\n\npython\nfrom sklearn.cluster import KMeans\nimport numpy as np\nX = np.array([[5, 5], [6, 5], [10, 8], [10, 12]])\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\nkmeans.labels_\n\n\nOutput: array([0, 0, 1, 1], dtype=int32)",
        "question": "Considering the above code, Which of the following do you think as correct output of Print(kmeans.predict([[6, 5]]))",
        "options": {
            "A": "0",
            "B": "1",
            "C": "5",
            "D": "10",
            "E": "None of these"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code and its output:\nkeep following symbols in mind:\n\n- >>> : Represents input code\n- # : Represents comment in a code\n- ... : Represents code continuation\n- Without any symbols at the beginning of a line then it is output of just above input line of code.\n\npython\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import SGDClassifier\n\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = SGDClassifier(random_state=0).fit(X, y)\n\n>>> print(y[70:80])\n[1 1 0 1 1 1 1 1 1 0]\n\n>>> print(clf.predict(X[70:80, :]))\n[0 1 1 1 1 1 1 0 1 0]\n\n>>> print(clf.score(X[70:80, :], y[70:80]))\n",
        "question": "What will be the output of the above code? Enter your answer correct to one decimal place.",
        "options": null,
        "correctOption": 0.7,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "python\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train = [[1,100],[4,400],[5,500],[6,600],[9,900],\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [1,100],[12,1200],[15,1500],[18,1800],[19,1900]]\ny_train = [2,2,2,2,2,2,2,1,1,1]\n\nX_test = [[2,200]]\n\nknn = KNeighborsClassifier(n_neighbors= 7,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric=\"euclidean\",\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0weights=\"uniform\")\n\nknn.fit(X_train,y_train)\n\nprint(knn.predict(X_test))\n",
        "question": "What will be the output of the following code?",
        "options": null,
        "correctOption": 2,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Consider the following code.",
        "question": "How many DecisionTreeClassifier models will be trained internally?\n\npython\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [{'max_depth':range(1, 10, 2)}]\ngs = GridSearchCV(RandomForestClassifier(n_estimators=10),\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0param_grid,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0cv = 5)\ngs.fit(X,y)\n",
        "options": null,
        "correctOption": 250,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Consider the following code snippet:\n\npython\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX_train = [[1, 2], [3, 4], [5, 6]]\ny_train = [0, 1, 2]\n\nknn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(X_train, y_train)\n",
        "question": "What will be the output of the following code:\n\npython\nprint(len(knn.classes_))\n",
        "options": null,
        "correctOption": 3,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Consider the following block of code:\n\npython\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX,y = load_breast_cancer(as_frame = True,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return_X_y = True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0test_size = 0.2,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0random_state = 1)\n\nclf = DecisionTreeClassifier(min_samples_split = 5,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 min_samples_leaf = 3,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 random_state = 5)\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\n",
        "question": "In which of the following scenarios, the split will NOT be made at node N?",
        "options": {
            "A": "Number of samples at node N = 10. If it is split, it will result in 5 nodes in the left child and 5 nodes in the right child.",
            "B": "Number of samples at node N = 5. If it is split, it will result in 2 nodes in the left child and 3 nodes in the right child.",
            "C": "Number of samples at node N = 12. If it is split, it will result in 5 nodes in the left child and 7 nodes in the right child.",
            "D": "Number of samples at node N = 4. If it is split, it will result in 3 nodes in the left child and 1 node in the right child."
        },
        "correctOption": [
            "B",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Consider the following block of code for the binary classification dataset. Shape of feature matrix is (10000,4) and labels (10000,) respectively.\n\nkeep following symbols in mind:\n\n- >>> : Represents input code\n- # : Represents comment in a code\n- ... : Represents code continuation\n- Without any symbols at the beginning of a line then it is output of just above input line of code.\n\npython\n>>> from sklearn.linear_model import LogisticRegression,SGDClassifier\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import VotingClassifier\n\n>>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n>>> clf2 = SGDClassifier(random_state=1)\n>>> clf3 = GaussianNB()\n\n>>> eclf = VotingClassifier(estimators=[('lr', clf1),\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0('sgd', clf2),\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0('gnb', clf3)],\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0voting='soft')\n\n>>> eclf.fit(X,y)\n\n>>> eclf.named_estimators_['lr'].predict_proba(X[0:1])\n[0.9,0.1]\n>>> eclf.named_estimators_['sgd'].predict_proba(X[0:1])\n[0.35,0.65]\n>>> eclf.named_estimators_['gnb'].predict_proba(X[0:1])\n[0.9,0.1]\n",
        "question": "what will be the predicted class for X[0:1] sample using the code given above",
        "options": {
            "A": "0",
            "B": "1",
            "C": "2",
            "D": "3"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "Given the following code using BaggingClassifier with KNeighborsClassifier as the base estimator:\n\npython\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nbase_knn = KNeighborsClassifier(n_neighbors=3,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0weights='distance')\n\nbag_clf = BaggingClassifier(base_knn,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0n_estimators=30, max_samples=100,\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0bootstrap=False, random_state=42)\n",
        "question": "Which of the following statements is correct?",
        "options": {
            "A": "Above code uses bootstrapping to generate samples for each base classifier.",
            "B": "Each base KNN classifier will be trained on a random subset of unknown number of samples.",
            "C": "Due to weights='distance', nearest neighbors in each base KNN classifier will have voting power inversely proportional to their distances from the sample (sample from test set).",
            "D": "The ensemble will consist of 3 base KNN classifiers."
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code.",
        "question": "How many different parameter combinations will be tried in GridSearchCV?\n\npython\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(as_frame = True, return_X_y = True)\n\nparam_grid = [{'max_depth':range(1, 10, 2),\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'min_samples_split': range(2, 10, 3)},\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0{'min_samples_leaf': range(1, 11, 3)}]\n\ngs = GridSearchCV(DecisionTreeClassifier(),\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0param_grid, cv = 5)\ngs.fit(X,y)\n",
        "options": null,
        "correctOption": 19,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Please consider the following data and code for a regression problem with following symbols in mind:\n\n- >>> : Represents input code\n- # : Represents comment in a code\n- ... : Represents code continuation\n- Without any symbols at the beginning of a line then it is output of just above input line of code.\n\nBased on the below data, answer the given subquestions.",
        "question": "Predict number of Accidents per 1000 Driver to happen for Age 28 and driving red car ?",
        "options": null,
        "correctOption": {
            "min": 91.0,
            "max": 93.0
        },
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764054533/sept2023-2_zw3wtd.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Please consider the following data and code for a regression problem with following symbols in mind:\n\n- >>> : Represents input code\n- # : Represents comment in a code\n- ... : Represents code continuation\n- Without any symbols at the beginning of a line then it is output of just above input line of code.\n\nBased on the below data, answer the given subquestions.",
        "question": "To improve the given Linear Regression model which of the following preprocessing step you suggest?",
        "options": {
            "A": "OrdinalEncoder()",
            "B": "DecisionTree()",
            "C": "LabelEncoder()",
            "D": "StandardScaler()"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764054533/sept2023-2_zw3wtd.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "Consider the following code snippet and its output:\n\nCode:\npython\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nX, y = make_classification(n_samples=3000, random_state=1)\nX_t, X_test, y_t, y_test = train_test_split(X, y, stratify=y, random_state=1)\nprint(y_test[30:35])\n\nOutput:\n`[0 0 0 1 0]`\n\nCode:\npython\nclf = MLPClassifier(random_state=1).fit(X_t, y_t)\nprint(clf.predict_proba(X_test[30:35]))\n\nOutput:\n`[[9.97710379e-01 2.28962074e-03]\n [3.5941505e-01 6.40528495e-01]\n [9.9405673e-01 5.94326977e-03]\n [1.7440866e-03 9.98255951e-01]\n [9.86367828e-01 1.36321719e-02]]`\n\nBased on the above data, answer the given subquestions.",
        "question": "What will be the output of following code?\n`print(clf.predict(X_test[30:35]))`",
        "options": {
            "A": "[1 1 0 1 0]",
            "B": "[0 0 0 1 0]",
            "C": "[0 1 0 0 0]",
            "D": "[0 1 0 1 0]"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2023",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider the following code snippet and its output:\n\nCode:\npython\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nX, y = make_classification(n_samples=3000, random_state=1)\nX_t, X_test, y_t, y_test = train_test_split(X, y, stratify=y, random_state=1)\nprint(y_test[30:35])\n\nOutput:\n`[0 0 0 1 0]`\n\nCode:\npython\nclf = MLPClassifier(random_state=1).fit(X_t, y_t)\nprint(clf.predict_proba(X_test[30:35]))\n\nOutput:\n`[[9.97710379e-01 2.28962074e-03]\n [3.5941505e-01 6.40528495e-01]\n [9.9405673e-01 5.94326977e-03]\n [1.7440866e-03 9.98255951e-01]\n [9.86367828e-01 1.36321719e-02]]`\n\nBased on the above data, answer the given subquestions.",
        "question": "What will be the output of following code?\n`print(clf.score(X_test[30:35], y_test[30:35]))`",
        "options": null,
        "correctOption": {
            "min": 0.78,
            "max": 0.82
        },
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following common data and answer the subquestions:\n\nimport pandas as pd\nimport numpy as np\ncolumns = [\"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\", \"Age\"]\ndata = [[148, 72, 0, 33.6, 50],\n \u00a0 \u00a0 \u00a0 \u00a0[85, 66, 0, 26.6, np.nan],\n \u00a0 \u00a0 \u00a0 \u00a0[183, 64, 0, 23.3, 32],\n \u00a0 \u00a0 \u00a0 \u00a0[89, 66, 94, 28.1, 21],\n \u00a0 \u00a0 \u00a0 \u00a0[137, 40, 168, 43.1, 33]]\ndf = pd.DataFrame(data=data, columns=columns)",
        "question": "What will be the output of the following code snippet?\n\nprint(df.shape[0]*df.shape[1] - df.isna().sum().sum())",
        "options": null,
        "correctOption": 24,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following common data and answer the subquestions:\n\nimport pandas as pd\nimport numpy as np\ncolumns = [\"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\", \"Age\"]\ndata = [[148, 72, 0, 33.6, 50],\n \u00a0 \u00a0 \u00a0 \u00a0[85, 66, 0, 26.6, np.nan],\n \u00a0 \u00a0 \u00a0 \u00a0[183, 64, 0, 23.3, 32],\n \u00a0 \u00a0 \u00a0 \u00a0[89, 66, 94, 28.1, 21],\n \u00a0 \u00a0 \u00a0 \u00a0[137, 40, 168, 43.1, 33]]\ndf = pd.DataFrame(data=data, columns=columns)",
        "question": "What will be the output of the following code snippet?\n\nt = df.Insulin.value_counts()\nprint(t.loc[0])",
        "options": null,
        "correctOption": 3,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following common data and answer the subquestions:\n\nimport pandas as pd\nimport numpy as np\ncolumns = [\"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\", \"Age\"]\ndata = [[148, 72, 0, 33.6, 50],\n \u00a0 \u00a0 \u00a0 \u00a0[85, 66, 0, 26.6, np.nan],\n \u00a0 \u00a0 \u00a0 \u00a0[183, 64, 0, 23.3, 32],\n \u00a0 \u00a0 \u00a0 \u00a0[89, 66, 94, 28.1, 21],\n \u00a0 \u00a0 \u00a0 \u00a0[137, 40, 168, 43.1, 33]]\ndf = pd.DataFrame(data=data, columns=columns)",
        "question": "What will be the output of the following code snippet?\n\nt= df.iloc[:,4]\nprint(t.mean())",
        "options": null,
        "correctOption": 34,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following common data and answer the subquestions:\n\nimport pandas as pd\nimport numpy as np\ncolumns = [\"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\", \"Age\"]\ndata = [[148, 72, 0, 33.6, 50],\n \u00a0 \u00a0 \u00a0 \u00a0[85, 66, 0, 26.6, np.nan],\n \u00a0 \u00a0 \u00a0 \u00a0[183, 64, 0, 23.3, 32],\n \u00a0 \u00a0 \u00a0 \u00a0[89, 66, 94, 28.1, 21],\n \u00a0 \u00a0 \u00a0 \u00a0[137, 40, 168, 43.1, 33]]\ndf = pd.DataFrame(data=data, columns=columns)",
        "question": "How many columns have datatype as \"float64\"?",
        "options": null,
        "correctOption": 2,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following common data and answer the subquestions:\n\nimport pandas as pd\nimport numpy as np\ncolumns = [\"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\", \"Age\"]\ndata = [[148, 72, 0, 33.6, 50],\n \u00a0 \u00a0 \u00a0 \u00a0[85, 66, 0, 26.6, np.nan],\n \u00a0 \u00a0 \u00a0 \u00a0[183, 64, 0, 23.3, 32],\n \u00a0 \u00a0 \u00a0 \u00a0[89, 66, 94, 28.1, 21],\n \u00a0 \u00a0 \u00a0 \u00a0[137, 40, 168, 43.1, 33]]\ndf = pd.DataFrame(data=data, columns=columns)",
        "question": "What will be the output of the following code snippet?\n\nt=(df\n.query(\"Glucose > 100\")\n.head(2)\n.tail(1)\n.iloc[0,1])\nprint(t)",
        "options": null,
        "correctOption": 64,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider following parameter grid for regression with RandomForest:\n\nparameter_grid = [\n \u00a0{'n_estimators': [10,20,30],\n \u00a0 'max_features': [2,3,4,5,6]},\n \u00a0{'bootstrap': [True, False],\n \u00a0 'min_samples_leaf': [1,2,3]}\n]",
        "question": "How many unique combinations of hyperparameters are there in the above parameter grid?",
        "options": null,
        "correctOption": 21,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "How to make SGDRegressor stop after 1000 epochs?",
        "options": {
            "A": "from sklearn.linear_model import SGDRegressor\nlinear_regressor = SGDRegressor(max_epoch=1000)",
            "B": "from sklearn.linear_model import SGDRegressor\nlinear_regressor = SGDRegressor(stopping_criteria=1000)",
            "C": "from sklearn.linear_model import SGDRegressor\nlinear_regressor = SGDRegressor(max_iter=1000)",
            "D": "from sklearn.linear_model import SGDRegressor\nlinear_regressor = SGDRegressor(stop_after_iter=1000)"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "Consider following data:\n\ndata = [{'age': 4, 'height':96.0},\n \u00a0 \u00a0 \u00a0 \u00a0{'age': 1, 'height':73.9},\n \u00a0 \u00a0 \u00a0 \u00a0{'age': 3, 'height':88.9},\n \u00a0 \u00a0 \u00a0 \u00a0{'age': 2, 'height':81.6}]",
        "question": "Which one of the following APIs can be used to extract features from the above data?",
        "options": {
            "A": "DictVectorizer",
            "B": "HashingVectorizer",
            "C": "FeatureHasher"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Which of the following is an example of an ordinal variable?",
        "options": {
            "A": "Blood type (A, B, AB, O)",
            "B": "Satisfaction rating (Very Unsatisfied, Unsatisfied, Neutral, Satisfied, Very Satisfied)",
            "C": "Height of individuals",
            "D": "Gender"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following method is used to find the predicted probability of each class of training samples using a trained model = SGDClassifier() ?",
        "options": {
            "A": "model.predict_proba(X_train)",
            "B": "model.predict(X_train)",
            "C": "model.estimate_",
            "D": "model.predict_proba_"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Consider following code snippet:\n\nestimator = RidgeClassifier(normalize=False, ____=0)\npipe_ridge = make_pipeline(MinMaxScaler(),estimator)\npipe_ridge.fit(x,y)",
        "question": "If we want to apply the ridge classifier on X with no regularization, what will be the missing attribute.",
        "options": {
            "A": "cv",
            "B": "reg_rate",
            "C": "alpha",
            "D": "tol"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nloocv = LeaveOneOut()\nscore = cross_val_score(lin_reg, X, y, cv=loocv)",
        "question": "For a dataset with 1000 data points and 100 features, the following code will generate how many models during execution?\nNote: X is the feature matrix and y is the target vector.",
        "options": {
            "A": "1000",
            "B": "100",
            "C": "99",
            "D": "999"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nlasso = make_pipeline(StandardScaler(), ____)\nlasso.fit(X_train, y_train)",
        "question": "Which of the following code snippets correctly applies Lasso regression with a regularization strength of 0.1 on scaled dataset?",
        "options": {
            "A": "Lasso(alpha=0.5)",
            "B": "Lasso(alpha=0.1)",
            "C": "Lasso(alpha=0.01, max_iter=500)",
            "D": "Lasso(alpha=1.0, tol=0.01)"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": "from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)",
        "question": "Consider the following polynomial regression model. What is the effect of the include_bias parameter?",
        "options": {
            "A": "It determines whether to include a bias term in the model.",
            "B": "It sets the degree of the polynomial features.",
            "C": "It controls the regularization strength.",
            "D": "It specifies the number of features to consider when fitting the model."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth=3, random_state=0)\nclf.fit(X_train, y_train)",
        "question": "Consider the following code snippet for training a decision tree classifier. What is the purpose of the max_depth parameter?",
        "options": {
            "A": "To specify the maximum number of samples per leaf.",
            "B": "To limit the depth of the tree.",
            "C": "To set the minimum number of samples required to split an internal node.",
            "D": "To control the minimum impurity decrease required for a split."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.tree import DecisionTreeRegressor\nreg = DecisionTreeRegressor(criterion='mse', random_state=42)\nreg.fit(X_train, y_train)",
        "question": "Given the code below, which of the following statements is true about the criterion parameter?",
        "options": {
            "A": "It determines the function to measure the quality of a split.",
            "B": "It specifies the number of features to consider when looking for the best split.",
            "C": "It sets the strategy used to choose the split at each node.",
            "D": "It controls whether to use the Gini impurity or entropy."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.ensemble import RandomForestClassifier\nclf = ____",
        "question": "Which of the following code snippets correctly initializes a RandomForestClassifier with 100 trees and bootstrapping enabled?",
        "options": {
            "A": "RandomForestClassifier(n_estimators=100, bootstrap=False)",
            "B": "RandomForestClassifier(estimators=100, bootstrap=False)",
            "C": "RandomForestClassifier(estimators_number=100, bootstrap=True)",
            "D": "RandomForestClassifier(n_estimators=100, bootstrap=True)"
        },
        "correctOption": "D",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": null,
        "question": "Consider below statements and choose the correct option:\nStatement1:The bagging technique combines multiple models trained on different subsets of data\nStatement2:The Boosting technique trains the model Parallely, focusing on the error made by the other model.",
        "options": {
            "A": "Statement1 is True, Statement2 is True",
            "B": "Statement1 is True, Statement2 is False",
            "C": "Statement1 is False, Statement2 is True",
            "D": "Statement1 is False, Statement2 is False"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "How does agglomerative clustering handle outliers?",
        "options": {
            "A": "It ignores outliers during the clustering process.",
            "B": "It assigns outliers to the nearest cluster.",
            "C": "It creates separate clusters for outliers.",
            "D": "It removes outliers from the dataset before clustering."
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "What is agglomerative clustering?",
        "options": {
            "A": "A hierarchical clustering technique that starts with each data point as its cluster and merges the closest clusters iteratively.",
            "B": "A method for partitioning data into a predefined number of clusters.",
            "C": "A clustering algorithm that uses centroids to iteratively assign data points to clusters.",
            "D": "A dimensionality reduction technique that projects data onto a lower-dimensional space."
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "In K-Means clustering, what does the inertia_ attribute represent?",
        "options": {
            "A": "The distance between cluster centroids",
            "B": "The number of clusters formed",
            "C": "The within-cluster sum of squared distances",
            "D": "The silhouette coefficient"
        },
        "correctOption": "C",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Consider the following ML task/steps for a regression dataset:\n1. Read the data from a file (named 'dataset.csv'). It has 7 columns. The last column is the target variable, all 6 features numerical.\n2. Remove rows which has target values missing.\n3. Split the data into training and test sets. Take randomly the 70% of rows in the training set and the rest of them into the test set.\n4. Fill the missing values in the features by KNNImputer using 3 nearest neighbours.\n5. Train a LinearRegression model on the training set.\n6. Report R2 score on the test set.\nWhich of the following code snippets correctly accomplishes the above task? Assume necessary imports.",
        "options": {
            "A": "data = pd.read_csv('dataset.csv')\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX = X[y.notna()]\ny = y.dropna()\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 3)), ('estimator', LinearRegression())])\npipe.fit(X_train,y_train)\nprint(pipe.score(X_test,y_test))",
            "B": "data = pd.read_csv('dataset.csv')\ndata.dropna(inplace=True)\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX = X[y.notna()]\ny = y.dropna()\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 3)), ('estimator', LinearRegression())])\npipe.fit(X_train,y_train)\nprint(pipe.score(X_test,y_test))",
            "C": "data = pd.read_csv('dataset.csv')\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 3)), ('estimator', LinearRegression())])\npipe.fit(X_train,y_train)\nprint(pipe.score(y_test,y_test))",
            "D": "data = pd.read_csv('dataset.csv')\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX = X[y.notna()]\ny = y.dropna()\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 5)), ('estimator', LinearRegression())])\npipe.fit(X_train,y_train)\nprint(pipe.score(X_test,y_test))",
            "E": "data = pd.read_csv('dataset.csv')\ndata.dropna()\nX, y = data[data.columns[:-1]],data[data.columns[-1]]\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\npipe = Pipeline([('imputer', KNNImputer(n_neighbors = 3)), ('estimator', LinearRegression())])\npipe.fit(X_train,y_train)\nprint(pipe.score(X_test,X_test))"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.datasets import load_wine\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX,y = load_wine(as_frame = True, return_X_y = True)\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\nclf1 = DecisionTreeClassifier(min_samples_split = 7, min_samples_leaf = 4, random_state = 5)\nclf1.fit(X_train, y_train)\nclf2 = DecisionTreeClassifier(min_samples_split = 4, min_samples_leaf = 2, random_state = 5)\nclf2.fit(X_train, y_train)",
        "question": "Consider two classifiers as shown in the following block of code:\nWhich of the following option is True?",
        "options": {
            "A": "clf1.tree_.max_depth >= clf2.tree_.max_depth",
            "B": "clf1.tree_.max_depth <= clf2.tree_.max_depth",
            "C": "clf1.tree_.max_depth == clf2.tree_.max_depth",
            "D": "Insufficient Information"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nbase_knn = KNeighborsClassifier(n_neighbors=3, weights='distance')\nbag_clf = BaggingClassifier(base_knn, n_estimators=30, max_samples=100, bootstrap=False, random_state=42)",
        "question": "Given the following code using BaggingClassifier with KNeighborsClassifier as the base estimator:\nWhich of the following statements is correct?",
        "options": {
            "A": "Above code uses bootstrapping to generate samples for each base classifier.",
            "B": "model will be tested on out of the bags samples.",
            "C": "Due to weights='distance', each base KNN classifier will treat all neighbors equally in terms of voting power.",
            "D": "The ensemble will consist of 3 base KNN classifiers.",
            "E": "None of these"
        },
        "correctOption": "E",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Decision Trees & Ensemble Methods (Random Forest, Boosting)",
        "context": "from sklearn.ensemble import VotingClassifier\nclf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = SVC(probability=True)\neclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')\neclf.fit(X_train, y_train)",
        "question": "Consider the following code for a VotingClassifier. What is the effect of setting voting='soft'?",
        "options": {
            "A": "The predictions are based on the majority vote.",
            "B": "The predictions are based on the average of probabilities predicted by each classifier.",
            "C": "The predictions are based on the weighted sum of the predictions.",
            "D": "The predictions are based on the classifier with the highest accuracy"
        },
        "correctOption": "B",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": "km = KMeans(n_clusters=5, init='random', n_init=10, random_state=42)\nkm.fit(X)",
        "question": "Suppose that we use Kmeans clustering for a dataset having 100 samples. The initial centroids for k clusters can be initialized in multiple ways. One such way is shown below. Choose the correct statements",
        "options": {
            "A": "5 centroids are randomly initialized 10 times",
            "B": "10 centroids are randomly initialized 5 times",
            "C": "5 samples in the dataset are selected as initialization point such that they are at least 10 units away from each other",
            "D": "10 samples in the dataset are selected as initialization point such that they are at least 5 units away from each other"
        },
        "correctOption": "A",
        "questionType": "single",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Data snooping:",
        "options": {
            "A": "Leads to biased estimation on test sets",
            "B": "Increases the risk of false positives",
            "C": "Leads to better estimation on training sets",
            "D": "Reduces the risk of false positives"
        },
        "correctOption": [
            "A",
            "B"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Data Preprocessing & Pandas Operations",
        "context": null,
        "question": "Which of the following algorithms may get impacted by feature scaling?",
        "options": {
            "A": "LinearRegression",
            "B": "DecisionTree",
            "C": "SVM",
            "D": "NaiveBayes"
        },
        "correctOption": [
            "A",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(class_weight='balanced', C=0.5)\nmodel.fit(X, y)",
        "question": "Consider the following code snippet that employs LogisticRegression from sklearn on a feature matrix X and corresponding label vector y:\nGiven the code above, which of the following statements is true?",
        "options": {
            "A": "The logistic regression model will use equal weights for both the classes in an imbalanced dataset.",
            "B": "The model does not use any regularization because the parameter C is set.",
            "C": "The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data.",
            "D": "The value of C indicates that the model will apply a regularization."
        },
        "correctOption": [
            "C",
            "D"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which of the following statements are true?",
        "options": {
            "A": "KNeighborsClassifier with high values of n_neighbors produces complex decision boundaries.",
            "B": "KNeighborsClassifier with high values of n_neighbors produces smooth decision boundaries.",
            "C": "In KNeighborsClassifier the scale of the features(columns) can impact the decision boundaries.",
            "D": "None of these"
        },
        "correctOption": [
            "B",
            "C"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": null,
        "question": "Which options are correct for Support Vectors in SVM ?",
        "options": {
            "A": "Support vectors are the data points nearest to the hyperplane",
            "B": "Using these support vectors, we maximize the margin of the classifier.",
            "C": "Using these support vectors, we minimize the margin of the classifier.",
            "D": "None of these"
        },
        "correctOption": [
            "A",
            "B"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "context": null,
        "question": "Which of the following are correct for Kmeans clustering algorithm?",
        "options": {
            "A": "It only finds spherical clusters.",
            "B": "The output of the algorithms depends upon initial cluster centroids.",
            "C": "It can automatically detect most appropriate value of k.",
            "D": "None of these."
        },
        "correctOption": [
            "A",
            "B"
        ],
        "questionType": "multiple",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": null,
        "question": "Using the confusion matrix given below. What is the precision score for the label (class) 1?",
        "options": null,
        "correctOption": 0.25,
        "questionType": "numerical",
        "image": "https://res.cloudinary.com/dnzudjm0y/image/upload/v1764054743/sept2024-1_w4nao5.png"
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.linear_model import Perceptron\n# Sample data\nX = [[0, 1], [0, 2], [2, 0], [2, 2]]\ny = [1, 1, 1, 2]\nclf = Perceptron(tol=None, shuffle=False)\nclf.fit(X, y)",
        "question": "What will be the output of the following code snippet?\nprint(clf.predict([[0, 1.5]]))",
        "options": null,
        "correctOption": 1,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Supervised Learning Algorithms (SVM, KNN, Naive Bayes, SGD)",
        "context": "from sklearn.neighbors import KNeighborsClassifier\n\nX_train = [[1, 2], [3, 4], [5, 6]]\ny_train = [0, 1, 2]\n\nknn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(X_train, y_train)",
        "question": "What will be the output of the following code:\n\nprint(len(knn.classes_))",
        "options": null,
        "correctOption": 3,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Let X be a feature matrix with shape (1000,5) and y be the label vector with two classes: 0 and 1. Assume that 650 examples of training data belong to class 1. Consider following code:\n\nbase_clf = DummyClassifier(strategy='most_frequent')\nbase_clf.fit(X,y)\n\nBased on the above data, answer the given subquestions.",
        "question": "What will be the output of the following code?\n\nprint(recall_score(y, base_clf.predict(X)))",
        "options": null,
        "correctOption": 1,
        "questionType": "numerical",
        "image": null
    },
    {
        "subject": "MLP",
        "exam": "ET",
        "term": "Sept 2024",
        "topic": "Model Evaluation, Tuning & Regularization",
        "context": "Let X be a feature matrix with shape (1000,5) and y be the label vector with two classes: 0 and 1. Assume that 650 examples of training data belong to class 1. Consider following code:\n\nbase_clf = DummyClassifier(strategy='most_frequent')\nbase_clf.fit(X,y)\n\nBased on the above data, answer the given subquestions.",
        "question": "What will be the output of the following code?\n\nprint(precision_score(y, base_clf.predict(X)))",
        "options": null,
        "correctOption": {
            "min": 0.64,
            "max": 0.66
        },
        "questionType": "numerical",
        "image": null
    }
]